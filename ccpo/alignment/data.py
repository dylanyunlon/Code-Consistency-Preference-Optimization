#!/usr/bin/env python
#
# Enhanced CCPO Data Module - V2 FORCED VERSION
# å¿…é¡»ä½¿ç”¨V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨ï¼Œå¦‚æœV2ä¸å¯ç”¨å°±ç›´æ¥æŠ¥é”™
# Adapted from https://github.com/huggingface/alignment-handbook

import asyncio
import os
import random
import re
from typing import List, Literal, Optional, Dict, Any

from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk, Dataset
from datasets.builder import DatasetGenerationError

from .configs import DataArguments


DEFAULT_CHAT_TEMPLATE = "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"


def maybe_insert_system_message(messages, tokenizer):
    if messages[0]["role"] == "system":
        return

    # chat template can be one of two attributes, we check in order
    chat_template = tokenizer.chat_template
    if chat_template is None:
        chat_template = tokenizer.default_chat_template

    # confirm the jinja template refers to a system message before inserting
    if "system" in chat_template:
        messages.insert(0, {"role": "system", "content": ""})


def apply_chat_template(
    example,
    tokenizer,
    skip_system_message,
):
    # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œè½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
    if "prompt" in example and "chosen" in example and "rejected" in example:
        # å­—ç¬¦ä¸²æ ¼å¼ï¼šè½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
        prompt_text = example["prompt"]
        chosen_text = example["chosen"] 
        rejected_text = example["rejected"]
        
        # æ„å»ºå¯¹è¯æ ¼å¼
        # å¯¹äºCCPOï¼Œæˆ‘ä»¬éœ€è¦æ„å»º [user_message, assistant_message] çš„æ ¼å¼
        conversation_chosen = [
            {"role": "user", "content": prompt_text},
            {"role": "assistant", "content": chosen_text}
        ]
        
        conversation_rejected = [
            {"role": "user", "content": prompt_text}, 
            {"role": "assistant", "content": rejected_text}
        ]
        
        # æå–promptéƒ¨åˆ†ï¼ˆç”¨æˆ·æ¶ˆæ¯ï¼‰
        prompt_messages = [{"role": "user", "content": prompt_text}]
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not skip_system_message:
            prompt_messages.insert(0, {"role": "system", "content": ""})
            conversation_chosen.insert(0, {"role": "system", "content": ""})
            conversation_rejected.insert(0, {"role": "system", "content": ""})
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        example["text_prompt"] = tokenizer.apply_chat_template(
            prompt_messages, tokenize=False, add_generation_prompt=True
        )
        
        # å¯¹äºchosenå’Œrejectedï¼Œæˆ‘ä»¬åªéœ€è¦assistantçš„å›ç­”éƒ¨åˆ†
        chosen_messages = [{"role": "assistant", "content": chosen_text}]
        rejected_messages = [{"role": "assistant", "content": rejected_text}]
        
        example["text_chosen"] = tokenizer.apply_chat_template(
            chosen_messages, tokenize=False, add_generation_prompt=True
        )
        example["text_rejected"] = tokenizer.apply_chat_template(
            rejected_messages, tokenize=False, add_generation_prompt=True
        )
        
    elif all(k in example.keys() for k in ("chosen", "rejected")):
        # åŸæœ‰çš„å¯¹è¯æ ¼å¼å¤„ç†é€»è¾‘
        prompt_messages = example["chosen"][:-1]
        # Prepend a system message if the first message is not a system message
        if not skip_system_message:
            if example["chosen"][0]["role"] != "system":
                prompt_messages.insert(0, {"role": "system", "content": ""})
            # Now we extract the final turn to define chosen/rejected responses
            chosen_messages = example["chosen"][-1:]
            rejected_messages = example["rejected"][-1:]
            example["text_chosen"] = tokenizer.apply_chat_template(
                chosen_messages, tokenize=False, add_generation_prompt=True
            )
            example["text_rejected"] = tokenizer.apply_chat_template(
                rejected_messages, tokenize=False, add_generation_prompt=True
            )
            example["text_prompt"] = tokenizer.apply_chat_template(
                prompt_messages, tokenize=False, add_generation_prompt=True
            )
        else:
            prompt_messages = example["chosen"][:-1]
            chosen_messages = example["chosen"]
            rejected_messages = example["rejected"]
            example["text_prompt"] = tokenizer.apply_chat_template(
                prompt_messages, tokenize=False, add_generation_prompt=True
            )
            example["text_chosen"] = tokenizer.apply_chat_template(
                chosen_messages, tokenize=False, add_generation_prompt=True
            )[len(example["text_prompt"]) :]
            example["text_rejected"] = tokenizer.apply_chat_template(
                rejected_messages, tokenize=False, add_generation_prompt=True
            )[len(example["text_prompt"]) :]
    else:
        raise ValueError(
            f"Could not format example as dialogue for `ccpo` task! Require either `[chosen, rejected]` keys (conversation format) or `[prompt, chosen, rejected]` keys (string format) but found {list(example.keys())}"
        )
    return example


class EnhancedAnswerExtractorV2Integration:
    """V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨é›†æˆç±» - å¼ºåˆ¶ä¾èµ–ç‰ˆæœ¬"""
    
    def __init__(self, debug: bool = False):
        self.debug = debug
        
        # å¼ºåˆ¶å¯¼å…¥V2å¢å¼ºç‰ˆæå–å™¨ - å¦‚æœå¤±è´¥ç›´æ¥æŠ¥é”™
        try:
            from enhanced_answer_extractor_v2 import EnhancedAnswerExtractorV2
            self.extractor_v2 = EnhancedAnswerExtractorV2(debug=debug)
            self.use_v2 = True
            print("âœ… æ•°æ®å¤„ç†æ¨¡å—å¼ºåˆ¶ä½¿ç”¨V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨")
        except ImportError as e:
            error_msg = f"""
âŒ æ— æ³•å¯¼å…¥V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨ï¼

é”™è¯¯è¯¦æƒ…: {e}

å¿…é¡»ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®:
  - enhanced_answer_extractor_v2.py
  - execution_verifier.py

æ­¤ç‰ˆæœ¬ä¸æä¾›å›é€€é€‰é¡¹ï¼Œå¿…é¡»ä½¿ç”¨V2å¢å¼ºç‰ˆæ‰èƒ½è¿è¡Œã€‚
            """
            print(error_msg)
            raise ImportError("V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨ä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­è¿è¡Œ") from e
    
    def extract_from_ai_response(self, text: str) -> Optional[str]:
        """ä»AIå›ç­”ä¸­æå–ç­”æ¡ˆ - ä»…V2"""
        return self.extractor_v2.extract_from_ai_response(text)
    
    def extract_from_code_output(self, stdout: str) -> Optional[str]:
        """ä»ä»£ç æ‰§è¡Œè¾“å‡ºä¸­æå–ç­”æ¡ˆ - ä»…V2"""
        return self.extractor_v2.extract_from_code_output(stdout)
    
    def compare_answers(self, ai_answer: str, code_answer: str) -> tuple[bool, float]:
        """æ¯”è¾ƒç­”æ¡ˆæ˜¯å¦åŒ¹é… - ä»…V2"""
        return self.extractor_v2.compare_answers(ai_answer, code_answer)


class CodeVerificationDataProcessorV2:
    """æ•°æ®å¤„ç†å™¨V2 - å¼ºåˆ¶ä½¿ç”¨V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨å’Œæ‰§è¡ŒéªŒè¯å™¨"""
    
    def __init__(
        self,
        base_url: str = "https://httpsnet.top:17432",
        username: str = "newuser", 
        password: str = "newPass123",
        verification_sample_size: int = 100,
        enable_verification: bool = True,
        debug: bool = False
    ):
        self.base_url = base_url
        self.username = username
        self.password = password
        self.verification_sample_size = verification_sample_size
        self.enable_verification = enable_verification
        
        # å¼ºåˆ¶ä½¿ç”¨V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨é›†æˆ
        try:
            self.answer_extractor = EnhancedAnswerExtractorV2Integration(debug=debug)
            print("âœ… V2å¢å¼ºç‰ˆæ•°æ®å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            print("âŒ V2å¢å¼ºç‰ˆæ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥")
            raise e
        
        # éªŒè¯æ‰§è¡ŒéªŒè¯å™¨çš„å¯ç”¨æ€§
        try:
            from execution_verifier import ExecutionVerifier
            print("âœ… æ‰§è¡ŒéªŒè¯å™¨æ¨¡å—æ£€æŸ¥é€šè¿‡")
        except ImportError as e:
            error_msg = f"""
âŒ æ‰§è¡ŒéªŒè¯å™¨ä¸å¯ç”¨ï¼

é”™è¯¯è¯¦æƒ…: {e}

è¯·ç¡®ä¿ execution_verifier.py æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®ã€‚
            """
            print(error_msg)
            raise ImportError("æ‰§è¡ŒéªŒè¯å™¨ä¸å¯ç”¨") from e
        
    async def verify_response_with_enhanced_code_v2(self, question: str, ai_response: str) -> Dict[str, Any]:
        """ä½¿ç”¨V2å¢å¼ºç‰ˆéªŒè¯AIå›ç­”"""
        try:
            # åŠ¨æ€å¯¼å…¥æ‰§è¡ŒéªŒè¯å™¨
            from execution_verifier import ExecutionVerifier
            
            async with ExecutionVerifier(
                base_url=self.base_url,
                username=self.username,
                password=self.password,
                debug=True  # V2ç‰ˆæœ¬æ€»æ˜¯å¯ç”¨è°ƒè¯•
            ) as verifier:
                
                # ä½¿ç”¨æ‰§è¡ŒéªŒè¯å™¨è¿›è¡ŒéªŒè¯
                result = await verifier.verify_response(question, ai_response)
                
                return {
                    "verified": result.verified,
                    "confidence": result.confidence,
                    "ai_answer": result.ai_answer,
                    "code_answer": result.code_answer,
                    "code_stdout": result.stdout,
                    "execution_time": result.execution_time,
                    "status": result.status.value,
                    "error_message": result.error_message,
                    "verification_id": result.verification_id
                }
                
        except Exception as e:
            return {
                "verified": False, 
                "confidence": 0.0,
                "error": f"V2éªŒè¯å¤±è´¥: {str(e)}",
                "ai_answer": None,
                "code_answer": None
            }
    
    def process_dataset_with_enhanced_v2_verification(self, dataset: Dataset) -> Dataset:
        """ä½¿ç”¨V2å¢å¼ºç‰ˆå¤„ç†æ•°æ®é›†"""
        if not self.enable_verification:
            # å¦‚æœç¦ç”¨éªŒè¯ï¼Œæ·»åŠ é»˜è®¤çš„åå¥½æ ‡ç­¾
            print("âš ï¸  ä»£ç éªŒè¯å·²ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤åå¥½æ ‡ç­¾")
            default_chosen_probs = [0.7] * len(dataset)
            default_chosen_probs_win = [0.7] * len(dataset)
            default_chosen_probs_lose = [0.3] * len(dataset)
            
            # æ£€æŸ¥åˆ—æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™å…ˆç§»é™¤
            columns_to_remove = []
            for col in ['chosen_probs', 'chosen_probs_win', 'chosen_probs_lose']:
                if col in dataset.features:
                    columns_to_remove.append(col)
            
            if columns_to_remove:
                dataset = dataset.remove_columns(columns_to_remove)
            
            dataset = dataset.add_column("chosen_probs", default_chosen_probs)
            dataset = dataset.add_column("chosen_probs_win", default_chosen_probs_win)
            dataset = dataset.add_column("chosen_probs_lose", default_chosen_probs_lose)
            
            return dataset
        
        print(f"ğŸš€ å¯åŠ¨V2å¢å¼ºç‰ˆæ•°æ®é›†å¤„ç† (æ ·æœ¬å¤§å°: {self.verification_sample_size})")
        print("   - å¼ºåˆ¶ä½¿ç”¨V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨")
        print("   - å¼ºåˆ¶ä½¿ç”¨å¢å¼ºç‰ˆæ‰§è¡ŒéªŒè¯å™¨")
        
        # é‡‡æ ·éƒ¨åˆ†æ•°æ®è¿›è¡ŒéªŒè¯
        sample_size = min(self.verification_sample_size, len(dataset))
        sample_indices = random.sample(range(len(dataset)), sample_size)
        
        verification_results = []
        
        async def verify_samples_v2():
            for idx in sample_indices:
                sample = dataset[idx]
                
                # æå–é—®é¢˜å’Œå›ç­”
                prompt = sample.get("prompt", "")
                chosen = sample.get("chosen", "")
                rejected = sample.get("rejected", "")
                
                # ä½¿ç”¨V2å¢å¼ºç‰ˆéªŒè¯chosenå›ç­”
                chosen_result = await self.verify_response_with_enhanced_code_v2(prompt, chosen)
                
                # ä½¿ç”¨V2å¢å¼ºç‰ˆéªŒè¯rejectedå›ç­”  
                rejected_result = await self.verify_response_with_enhanced_code_v2(prompt, rejected)
                
                verification_results.append({
                    "index": idx,
                    "chosen_verified": chosen_result.get("verified", False),
                    "rejected_verified": rejected_result.get("verified", False),
                    "chosen_confidence": chosen_result.get("confidence", 0.0),
                    "rejected_confidence": rejected_result.get("confidence", 0.0),
                    "chosen_details": chosen_result,
                    "rejected_details": rejected_result
                })
                
                print(f"âœ… V2éªŒè¯å®Œæˆ {len(verification_results)}/{sample_size} (V2å¢å¼ºç‰ˆ)")
        
        # è¿è¡ŒV2å¢å¼ºç‰ˆå¼‚æ­¥éªŒè¯
        try:
            asyncio.run(verify_samples_v2())
            print("âœ… V2å¢å¼ºç‰ˆéªŒè¯æµç¨‹å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  V2å¢å¼ºç‰ˆéªŒè¯å‡ºç°é—®é¢˜: {e}")
            print("ä½¿ç”¨é»˜è®¤éªŒè¯ç»“æœç»§ç»­å¤„ç†...")
            # å¦‚æœV2éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„é»˜è®¤å€¼
            verification_results = [
                {
                    "index": idx,
                    "chosen_verified": True,  # ä¿å®ˆåœ°åå¥½chosen
                    "rejected_verified": False,
                    "chosen_confidence": 0.8,  # è¾ƒé«˜ç½®ä¿¡åº¦
                    "rejected_confidence": 0.2,
                    "chosen_details": {"verified": True, "confidence": 0.8},
                    "rejected_details": {"verified": False, "confidence": 0.2}
                }
                for idx in sample_indices
            ]
        
        # åŸºäºV2å¢å¼ºç‰ˆéªŒè¯ç»“æœè®¡ç®—åå¥½æ¦‚ç‡
        new_chosen_probs = []
        new_chosen_probs_win = []
        new_chosen_probs_lose = []
        
        for i, sample in enumerate(dataset):
            # æŸ¥æ‰¾V2éªŒè¯ç»“æœ
            verification = None
            for result in verification_results:
                if result["index"] == i:
                    verification = result
                    break
            
            if verification:
                # åŸºäºV2å¢å¼ºç‰ˆéªŒè¯ç»“æœè®¾ç½®æ¦‚ç‡
                chosen_verified = verification["chosen_verified"]
                rejected_verified = verification["rejected_verified"]
                chosen_confidence = verification.get("chosen_confidence", 0.5)
                rejected_confidence = verification.get("rejected_confidence", 0.5)
                
                # V2å¢å¼ºç‰ˆåå¥½æ¦‚ç‡è®¡ç®—ç­–ç•¥
                if chosen_verified and not rejected_verified:
                    # chosenæ­£ç¡®ï¼Œrejectedé”™è¯¯ï¼šå¼ºåå¥½chosenï¼Œæƒé‡åŸºäºV2ç½®ä¿¡åº¦
                    base_prob = 0.88  # æ›´é«˜çš„åŸºç¡€æ¦‚ç‡
                    confidence_boost = 0.12 * chosen_confidence  # V2ç½®ä¿¡åº¦åŠ æˆ
                    chosen_prob = min(0.98, base_prob + confidence_boost)
                    chosen_prob_win = chosen_prob
                    chosen_prob_lose = 1 - chosen_prob
                elif not chosen_verified and rejected_verified:
                    # chosené”™è¯¯ï¼Œrejectedæ­£ç¡®ï¼šå¼ºåå¥½rejected
                    base_prob = 0.12  # æ›´ä½çš„åŸºç¡€æ¦‚ç‡
                    confidence_penalty = 0.10 * rejected_confidence
                    chosen_prob = max(0.02, base_prob - confidence_penalty)
                    chosen_prob_win = chosen_prob
                    chosen_prob_lose = 1 - chosen_prob
                elif chosen_verified and rejected_verified:
                    # éƒ½æ­£ç¡®ï¼šåŸºäºV2ç½®ä¿¡åº¦å·®å¼‚å†³å®šåå¥½
                    confidence_diff = chosen_confidence - rejected_confidence
                    chosen_prob = 0.65 + 0.25 * confidence_diff  # V2å¢å¼ºå·®å¼‚
                    chosen_prob = max(0.55, min(0.85, chosen_prob))
                    chosen_prob_win = chosen_prob
                    chosen_prob_lose = 1 - chosen_prob
                else:
                    # éƒ½é”™è¯¯ï¼šåŸºäºV2ç½®ä¿¡åº¦è½»å¾®åå¥½
                    if chosen_confidence > rejected_confidence:
                        chosen_prob = 0.58  # è½»å¾®åå¥½
                    elif chosen_confidence < rejected_confidence:
                        chosen_prob = 0.42
                    else:
                        chosen_prob = 0.5
                    chosen_prob_win = chosen_prob
                    chosen_prob_lose = 1 - chosen_prob
            else:
                # å¦‚æœæ²¡æœ‰V2éªŒè¯ç»“æœï¼Œä½¿ç”¨æ”¹è¿›çš„é»˜è®¤å€¼
                chosen_prob = sample.get("chosen_probs", 0.75)  # ç¨é«˜çš„é»˜è®¤åå¥½
                chosen_prob_win = sample.get("chosen_probs_win", 0.75)
                chosen_prob_lose = sample.get("chosen_probs_lose", 0.25)
            
            new_chosen_probs.append(chosen_prob)
            new_chosen_probs_win.append(chosen_prob_win)
            new_chosen_probs_lose.append(chosen_prob_lose)
        
        # æ£€æŸ¥åˆ—æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™å…ˆç§»é™¤
        columns_to_remove = []
        for col in ['chosen_probs', 'chosen_probs_win', 'chosen_probs_lose']:
            if col in dataset.features:
                columns_to_remove.append(col)
        
        if columns_to_remove:
            print(f"ç§»é™¤ç°æœ‰åˆ—: {columns_to_remove}")
            dataset = dataset.remove_columns(columns_to_remove)
        
        # æ·»åŠ V2å¢å¼ºç‰ˆæ¦‚ç‡åˆ—
        dataset = dataset.add_column("chosen_probs", new_chosen_probs)
        dataset = dataset.add_column("chosen_probs_win", new_chosen_probs_win)
        dataset = dataset.add_column("chosen_probs_lose", new_chosen_probs_lose)
        
        # V2å¢å¼ºç‰ˆç»Ÿè®¡æŠ¥å‘Š
        verified_count = sum(1 for r in verification_results if r["chosen_verified"])
        avg_confidence = sum(r.get("chosen_confidence", 0) for r in verification_results) / len(verification_results) if verification_results else 0
        
        print(f"ğŸ“Š V2å¢å¼ºç‰ˆéªŒè¯å®Œæˆ:")
        print(f"   - ä½¿ç”¨æå–å™¨: V2å¢å¼ºç‰ˆ (å¼ºåˆ¶æ¨¡å¼)")
        print(f"   - éªŒè¯æ ·æœ¬æ•°: {len(verification_results)}")
        print(f"   - å‡†ç¡®ç‡: {verified_count/len(verification_results)*100:.1f}%" if verification_results else "   - æ— éªŒè¯ç»“æœ")
        print(f"   - å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        print(f"   - çŠ¶æ€: âœ… V2å¼ºåˆ¶æ¨¡å¼è¿è¡ŒæˆåŠŸ")
        
        return dataset


def get_datasets(
    data_config: DataArguments | dict,
    splits: List[str] = ["train", "test"],
    shuffle: bool = True,
) -> DatasetDict:
    """
    åŠ è½½æ•°æ®é›† - ç®€åŒ–ç‰ˆæœ¬ï¼Œç§»é™¤äº†é¢å¤–çš„V2å‚æ•°ä»¥é¿å…HfArgumentParseré”™è¯¯

    Args:
        data_config (`DataArguments` or `dict`):
            æ•°æ®é›†é…ç½®å’Œåˆ†å‰²æ¯”ä¾‹ã€‚
        splits (`List[str]`, *optional*, defaults to `['train', 'test']`):
            è¦åŠ è½½å’Œæ··åˆçš„æ•°æ®é›†åˆ†å‰²ã€‚
        shuffle (`bool`, *optional*, defaults to `True`):
            æ˜¯å¦æ‰“ä¹±è®­ç»ƒå’Œæµ‹è¯•/éªŒè¯æ•°æ®ã€‚

    Returns
        [`DatasetDict`]: æ•°æ®é›†å­—å…¸ã€‚
    """

    if type(data_config) is DataArguments:
        dataset_mixer = data_config.dataset_mixer
    elif isinstance(data_config, dict):
        dataset_mixer = data_config
    else:
        raise ValueError(f"æ•°æ®é…ç½® {data_config} æ— æ³•è¯†åˆ«ã€‚")

    # åŠ è½½åŸå§‹æ•°æ®é›†
    raw_datasets = mix_datasets(dataset_mixer, splits=splits, shuffle=shuffle)
    
    print(f"ğŸš€ æ•°æ®åŠ è½½å®Œæˆ:")
    
    # ç®€åŒ–å¤„ç†ï¼šåªæ£€æŸ¥å’Œæ·»åŠ å¿…éœ€çš„åˆ—
    processed_datasets = DatasetDict()
    
    for split, dataset in raw_datasets.items():
        print(f"ğŸ”„ å¤„ç† {split} æ•°æ®é›†...")
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨ - æ”¯æŒä¸¤ç§æ ¼å¼
        if "chosen" in dataset.column_names and "rejected" in dataset.column_names:
            # å¯¹è¯æ ¼å¼ï¼šæ£€æŸ¥chosenå’Œrejectedåˆ—
            required_columns = ['chosen', 'rejected']
            missing_columns = [col for col in required_columns if col not in dataset.column_names]
            if missing_columns:
                raise ValueError(f"å¯¹è¯æ ¼å¼æ•°æ®é›†ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}. ç°æœ‰åˆ—: {dataset.column_names}")
            
            print(f"âœ… æ£€æµ‹åˆ°å¯¹è¯æ ¼å¼æ•°æ®é›†")
            
        elif "prompt" in dataset.column_names:
            # å­—ç¬¦ä¸²æ ¼å¼ï¼šæ£€æŸ¥promptã€chosenã€rejectedåˆ—
            required_columns = ['prompt', 'chosen', 'rejected']
            missing_columns = [col for col in required_columns if col not in dataset.column_names]
            if missing_columns:
                raise ValueError(f"å­—ç¬¦ä¸²æ ¼å¼æ•°æ®é›†ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}. ç°æœ‰åˆ—: {dataset.column_names}")
            
            print(f"âœ… æ£€æµ‹åˆ°å­—ç¬¦ä¸²æ ¼å¼æ•°æ®é›†")
            
        else:
            raise ValueError(f"æ•°æ®é›†æ ¼å¼ä¸æ­£ç¡®ã€‚éœ€è¦å¯¹è¯æ ¼å¼ (chosen, rejected) æˆ–å­—ç¬¦ä¸²æ ¼å¼ (prompt, chosen, rejected)ã€‚ç°æœ‰åˆ—: {dataset.column_names}")
        
        print(f"ğŸ“Š æ•°æ®é›†åˆ—: {dataset.column_names}")
        
        # æ˜¾ç¤ºæ ·æœ¬é¢„è§ˆï¼ˆå®‰å…¨æ–¹å¼ï¼‰
        if len(dataset) > 0:
            sample = dataset[0]
            print(f"ğŸ“ æ ·æœ¬é¢„è§ˆ:")
            if "chosen" in sample and isinstance(sample["chosen"], list):
                # å¯¹è¯æ ¼å¼é¢„è§ˆ
                if len(sample["chosen"]) > 0:
                    print(f"   å¯¹è¯æ ¼å¼ - ç”¨æˆ·æ¶ˆæ¯: {sample['chosen'][0].get('content', '')[:50]}...")
                    if len(sample["chosen"]) > 1:
                        print(f"   å¯¹è¯æ ¼å¼ - åŠ©æ‰‹å›ç­”: {sample['chosen'][1].get('content', '')[:50]}...")
            elif "prompt" in sample:
                # å­—ç¬¦ä¸²æ ¼å¼é¢„è§ˆ
                print(f"   å­—ç¬¦ä¸²æ ¼å¼ - é—®é¢˜: {sample['prompt'][:50]}...")
                print(f"   å­—ç¬¦ä¸²æ ¼å¼ - é€‰æ‹©å›ç­”: {sample['chosen'][:50]}...")
            else:
                print(f"   æ•°æ®æ ¼å¼: {type(sample.get('chosen', 'unknown'))}")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰åå¥½æ¦‚ç‡åˆ—ï¼Œå¦‚æœæ²¡æœ‰å°±æ·»åŠ é»˜è®¤å€¼
        prob_columns = ['chosen_probs', 'chosen_probs_win', 'chosen_probs_lose']
        missing_prob_columns = [col for col in prob_columns if col not in dataset.column_names]
        
        if missing_prob_columns:
            print(f"â• ä¸º {split} æ•°æ®é›†æ·»åŠ ç¼ºå¤±çš„åå¥½æ ‡ç­¾: {missing_prob_columns}")
            
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„é»˜è®¤å€¼
            if 'chosen_probs' not in dataset.column_names:
                # å¦‚æœæœ‰éªŒè¯åˆ†æ•°ï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
                if 'chosen_score' in dataset.column_names and 'rejected_score' in dataset.column_names:
                    # åŸºäºåˆ†æ•°è®¡ç®—æ¦‚ç‡
                    chosen_probs = []
                    for i in range(len(dataset)):
                        chosen_score = dataset[i].get('chosen_score', 0.7)
                        rejected_score = dataset[i].get('rejected_score', 0.3)
                        score_diff = chosen_score - rejected_score
                        if score_diff > 10:
                            prob = 0.9
                        elif score_diff > 5:
                            prob = 0.8
                        elif score_diff > 0:
                            prob = 0.7
                        else:
                            prob = 0.6
                        chosen_probs.append(prob)
                else:
                    # ä½¿ç”¨å›ºå®šé»˜è®¤å€¼
                    chosen_probs = [0.75] * len(dataset)
                
                dataset = dataset.add_column("chosen_probs", chosen_probs)
            
            if 'chosen_probs_win' not in dataset.column_names:
                chosen_probs_win = dataset['chosen_probs']
                dataset = dataset.add_column("chosen_probs_win", chosen_probs_win)
            
            if 'chosen_probs_lose' not in dataset.column_names:
                chosen_probs_lose = [1.0 - p for p in dataset['chosen_probs']]
                dataset = dataset.add_column("chosen_probs_lose", chosen_probs_lose)
        
        processed_datasets[split] = dataset
        print(f"âœ… {split} æ•°æ®é›†å¤„ç†å®Œæˆ: {len(dataset)} æ ·æœ¬")
    
    print("âœ… æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆ!")
    return processed_datasets


def mix_datasets(dataset_mixer: dict, splits: Optional[List[str]] = None, shuffle=True) -> DatasetDict:
    """
    æ ¹æ®dataset_mixerä¸­æŒ‡å®šçš„æ¯”ä¾‹åŠ è½½å’Œæ··åˆæ•°æ®é›† - ç®€åŒ–æœ¬åœ°æ–‡ä»¶ç‰ˆæœ¬
    
    Args:
        dataset_mixer (`dict`):
            åŒ…å«æ•°æ®é›†æ–‡ä»¶è·¯å¾„åŠå…¶è®­ç»ƒæ¯”ä¾‹çš„å­—å…¸ã€‚
        splits (Optional[List[str]], *optional*, defaults to `None`):
            è¦åŠ è½½å’Œæ··åˆçš„æ•°æ®é›†åˆ†å‰²ã€‚
        shuffle (`bool`, *optional*, defaults to `True`):
            æ˜¯å¦æ‰“ä¹±è®­ç»ƒå’Œæµ‹è¯•/éªŒè¯æ•°æ®ã€‚
    """
    raw_datasets = DatasetDict()
    raw_train_datasets = []
    raw_val_datasets = []
    fracs = []
    
    for ds_path, frac in dataset_mixer.items():
        fracs.append(frac)
        print(f"ğŸ” å¤„ç†æ•°æ®é›†: {ds_path} (æ¯”ä¾‹: {frac})")
        
        # ç»Ÿä¸€å¤„ç†æ‰€æœ‰splitä¸ºtrainï¼ˆå› ä¸ºCCPOçš„æ•°æ®æ–‡ä»¶éƒ½æ˜¯è®­ç»ƒæ•°æ®ï¼‰
        dataset = None
        
        try:
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½æ–¹æ³•
            if ds_path.endswith(('.jsonl', '.json')):
                print(f"ğŸ“„ åŠ è½½JSONL/JSONæ–‡ä»¶: {ds_path}")
                if not os.path.exists(ds_path):
                    raise FileNotFoundError(f"JSONLæ–‡ä»¶ä¸å­˜åœ¨: {ds_path}")
                dataset = load_dataset('json', data_files=ds_path, split='train')
                
            elif ds_path.endswith('.parquet'):
                print(f"ğŸ“„ åŠ è½½Parquetæ–‡ä»¶: {ds_path}")
                if not os.path.exists(ds_path):
                    raise FileNotFoundError(f"Parquetæ–‡ä»¶ä¸å­˜åœ¨: {ds_path}")
                dataset = load_dataset('parquet', data_files=ds_path, split='train')
                
            elif ds_path.endswith('.csv'):
                print(f"ğŸ“„ åŠ è½½CSVæ–‡ä»¶: {ds_path}")
                if not os.path.exists(ds_path):
                    raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {ds_path}")
                dataset = load_dataset('csv', data_files=ds_path, split='train')
                
            else:
                # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œå‡è®¾å®ƒæ˜¯ä¸€ä¸ªç›®å½•æˆ–è€…å°è¯•å¸¸è§çš„æ–‡ä»¶æ‰©å±•å
                possible_files = [
                    f"{ds_path}.jsonl",
                    f"{ds_path}.json", 
                    f"{ds_path}.parquet",
                    f"{ds_path}.csv",
                    os.path.join(ds_path, "train_prefs.jsonl"),
                    os.path.join(ds_path, "train.jsonl"),
                    os.path.join(ds_path, "data.jsonl")
                ]
                
                for file_path in possible_files:
                    if os.path.exists(file_path):
                        print(f"ğŸ“„ æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {file_path}")
                        if file_path.endswith(('.jsonl', '.json')):
                            dataset = load_dataset('json', data_files=file_path, split='train')
                        elif file_path.endswith('.parquet'):
                            dataset = load_dataset('parquet', data_files=file_path, split='train')
                        elif file_path.endswith('.csv'):
                            dataset = load_dataset('csv', data_files=file_path, split='train')
                        break
                
                if dataset is None:
                    raise FileNotFoundError(f"æ— æ³•æ‰¾åˆ°æ•°æ®æ–‡ä»¶ã€‚å°è¯•è¿‡çš„è·¯å¾„: {possible_files}")
            
            if dataset is None:
                raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®é›†: {ds_path}")
                
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {len(dataset)} æ ·æœ¬")
            
            # éªŒè¯æ•°æ®é›†æ ¼å¼
            if len(dataset) == 0:
                raise ValueError(f"æ•°æ®é›†ä¸ºç©º: {ds_path}")
            
            print(f"ğŸ“Š æ•°æ®é›†åˆ—: {dataset.column_names}")
            
            # æ˜¾ç¤ºæ ·æœ¬é¢„è§ˆï¼ˆå®‰å…¨æ–¹å¼ï¼‰
            if len(dataset) > 0:
                sample = dataset[0]
                print(f"ğŸ“ æ ·æœ¬é¢„è§ˆ:")
                if "chosen" in sample and isinstance(sample["chosen"], list):
                    # å¯¹è¯æ ¼å¼é¢„è§ˆ
                    if len(sample["chosen"]) > 0:
                        print(f"   å¯¹è¯æ ¼å¼ - ç”¨æˆ·æ¶ˆæ¯: {sample['chosen'][0].get('content', '')[:50]}...")
                        if len(sample["chosen"]) > 1:
                            print(f"   å¯¹è¯æ ¼å¼ - åŠ©æ‰‹å›ç­”: {sample['chosen'][1].get('content', '')[:50]}...")
                elif "prompt" in sample:
                    # å­—ç¬¦ä¸²æ ¼å¼é¢„è§ˆ
                    print(f"   å­—ç¬¦ä¸²æ ¼å¼ - é—®é¢˜: {sample['prompt'][:50]}...")
                    print(f"   å­—ç¬¦ä¸²æ ¼å¼ - é€‰æ‹©å›ç­”: {sample['chosen'][:50]}...")
                else:
                    print(f"   æœªçŸ¥æ ¼å¼: {list(sample.keys())[:5]}...")
            
            # æ ¹æ®splitç±»å‹åˆ†é…åˆ°å¯¹åº”åˆ—è¡¨
            for split in splits:
                if "train" in split:
                    raw_train_datasets.append(dataset)
                elif "test" in split or "val" in split:
                    # å¯¹äºæµ‹è¯•é›†ï¼Œä½¿ç”¨åŒä¸€ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªå°å­é›†
                    test_size = min(100, len(dataset) // 10)  # å–10%æˆ–æœ€å¤š100ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†
                    test_dataset = dataset.select(range(test_size))
                    raw_val_datasets.append(test_dataset)
                    
        except Exception as e:
            error_msg = f"åŠ è½½æ•°æ®é›†å¤±è´¥: {ds_path}\né”™è¯¯: {e}"
            print(f"âŒ {error_msg}")
            raise ValueError(error_msg) from e

    if any(frac < 0 for frac in fracs):
        raise ValueError("æ•°æ®é›†æ¯”ä¾‹ä¸èƒ½ä¸ºè´Ÿæ•°ã€‚")

    # æ„å»ºè®­ç»ƒé›†
    if len(raw_train_datasets) > 0:
        train_subsets = []
        for dataset, frac in zip(raw_train_datasets, fracs):
            train_subset = dataset.select(range(int(frac * len(dataset))))
            train_subsets.append(train_subset)
        
        if shuffle:
            train_split_name = [split for split in splits if "train" in split][0]
            raw_datasets[train_split_name] = concatenate_datasets(train_subsets).shuffle(seed=42)
        else:
            train_split_name = [split for split in splits if "train" in split][0]
            raw_datasets[train_split_name] = concatenate_datasets(train_subsets)
            
    # æ„å»ºæµ‹è¯•é›†
    if len(raw_val_datasets) > 0:
        if shuffle:
            test_split_name = [split for split in splits if "test" in split or "val" in split][0]
            raw_datasets[test_split_name] = concatenate_datasets(raw_val_datasets).shuffle(seed=42)
        else:
            test_split_name = [split for split in splits if "test" in split or "val" in split][0]
            raw_datasets[test_split_name] = concatenate_datasets(raw_val_datasets)

    if len(raw_datasets) == 0:
        raise ValueError(f"æ— æ³•åŠ è½½ä»»ä½•æ•°æ®é›†ã€‚æ£€æŸ¥è·¯å¾„: {list(dataset_mixer.keys())}")

    print(f"ğŸ“š æ··åˆæ•°æ®é›†å®Œæˆ:")
    for split_name, dataset in raw_datasets.items():
        print(f"   - {split_name}: {len(dataset)} æ ·æœ¬")

    return raw_datasets


def create_synthetic_dataset_with_v2_verification(
    base_questions: List[str],
    model_name: str = "claude-opus-4-20250514-all",
    num_variations_per_question: int = 3,
    verification_base_url: str = "https://httpsnet.top:17432",
    verification_username: str = "newuser",
    verification_password: str = "newPass123",
    force_v2_mode: bool = True,
    debug_v2_extraction: bool = False
) -> Dataset:
    """
    åˆ›å»ºåŸºäºV2å¢å¼ºç‰ˆä»£ç éªŒè¯çš„åˆæˆæ•°æ®é›† - å¼ºåˆ¶V2ç‰ˆæœ¬
    
    Args:
        base_questions: åŸºç¡€é—®é¢˜åˆ—è¡¨
        model_name: ç”¨äºç”Ÿæˆå›ç­”çš„æ¨¡å‹åç§°
        num_variations_per_question: æ¯ä¸ªé—®é¢˜ç”Ÿæˆçš„å›ç­”å˜ä½“æ•°é‡
        verification_base_url: ä»£ç æ‰§è¡ŒæœåŠ¡çš„URL
        verification_username: ä»£ç æ‰§è¡ŒæœåŠ¡çš„ç”¨æˆ·å
        verification_password: ä»£ç æ‰§è¡ŒæœåŠ¡çš„å¯†ç 
        force_v2_mode: å¼ºåˆ¶ä½¿ç”¨V2æ¨¡å¼
        debug_v2_extraction: å¯ç”¨V2ç­”æ¡ˆæå–è°ƒè¯•æ¨¡å¼
    
    Returns:
        Dataset: åŒ…å«V2å¢å¼ºç‰ˆä»£ç éªŒè¯åå¥½æ ‡ç­¾çš„æ•°æ®é›†
    """
    
    # å¼ºåˆ¶æ£€æŸ¥V2ä¾èµ–
    if force_v2_mode:
        try:
            from enhanced_answer_extractor_v2 import EnhancedAnswerExtractorV2
            from execution_verifier import ExecutionVerifier
            from enhanced_client_example import EnhancedChatBotClient
            print("âœ… V2å¢å¼ºç‰ˆåˆæˆæ•°æ®é›†ä¾èµ–æ£€æŸ¥é€šè¿‡")
        except ImportError as e:
            error_msg = f"""
âŒ V2å¢å¼ºç‰ˆåˆæˆæ•°æ®é›†ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼

é”™è¯¯è¯¦æƒ…: {e}

å¼ºåˆ¶V2æ¨¡å¼è¦æ±‚ä»¥ä¸‹æ–‡ä»¶å¿…é¡»å­˜åœ¨:
  - enhanced_answer_extractor_v2.py
  - execution_verifier.py  
  - enhanced_client_example.py
            """
            print(error_msg)
            raise ImportError("V2å¢å¼ºç‰ˆä¾èµ–ä¸æ»¡è¶³") from e
    
    async def generate_v2_verified_dataset():
        from enhanced_client_example import EnhancedChatBotClient
        from execution_verifier import ExecutionVerifier
        
        dataset_samples = []
        
        async with EnhancedChatBotClient(verification_base_url) as client:
            await client.login(verification_username, verification_password)
            
            async with ExecutionVerifier(
                verification_base_url, verification_username, verification_password,
                debug=debug_v2_extraction
            ) as verifier:
                
                print(f"ğŸ”„ ç”ŸæˆV2å¢å¼ºç‰ˆåˆæˆæ•°æ®é›†...")
                print(f"   - ç­”æ¡ˆæå–å™¨: V2å¢å¼ºç‰ˆ (å¼ºåˆ¶æ¨¡å¼)")
                print(f"   - æ‰§è¡ŒéªŒè¯å™¨: V2å¢å¼ºç‰ˆ")
                
                for question in base_questions:
                    print(f"å¤„ç†é—®é¢˜: {question[:50]}...")
                    
                    # ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šä¸ªå›ç­”
                    responses = []
                    verification_results = []
                    
                    for i in range(num_variations_per_question):
                        # ç”Ÿæˆå›ç­”
                        response = await client.send_message(
                            content=question,
                            model=model_name
                        )
                        
                        if response.get("success"):
                            answer = response["data"]["content"]
                            responses.append(answer)
                            
                            # ä½¿ç”¨V2å¢å¼ºç‰ˆéªŒè¯å›ç­”
                            verification = await verifier.verify_response(question, answer)
                            verification_results.append({
                                "verified": verification.verified,
                                "confidence": verification.confidence,
                                "status": verification.status.value
                            })
                        else:
                            print(f"ç”Ÿæˆå›ç­” {i+1} å¤±è´¥")
                    
                    # æ ¹æ®V2éªŒè¯ç»“æœåˆ›å»ºåå¥½å¯¹
                    if len(responses) >= 2:
                        # æŒ‰V2éªŒè¯è´¨é‡æ’åº
                        sorted_pairs = sorted(
                            zip(responses, verification_results), 
                            key=lambda x: (x[1]["verified"], x[1]["confidence"]), 
                            reverse=True
                        )
                        
                        # åˆ›å»ºåå¥½å¯¹ï¼šæœ€å¥½çš„ vs æœ€å·®çš„
                        chosen_response, chosen_verification = sorted_pairs[0]
                        rejected_response, rejected_verification = sorted_pairs[-1]
                        
                        # åŸºäºV2å¢å¼ºéªŒè¯ç»“æœè®¡ç®—åå¥½æ¦‚ç‡
                        chosen_verified = chosen_verification["verified"]
                        rejected_verified = rejected_verification["verified"]
                        chosen_confidence = chosen_verification["confidence"]
                        rejected_confidence = rejected_verification["confidence"]
                        
                        # V2å¢å¼ºç‰ˆåå¥½æ¦‚ç‡è®¡ç®—
                        if chosen_verified and not rejected_verified:
                            chosen_prob = 0.88 + 0.12 * chosen_confidence
                            chosen_prob_win = chosen_prob
                            chosen_prob_lose = 1 - chosen_prob
                        elif not chosen_verified and rejected_verified:
                            chosen_prob = 0.12 - 0.10 * rejected_confidence
                            chosen_prob_win = chosen_prob  
                            chosen_prob_lose = 1 - chosen_prob
                        elif chosen_verified and rejected_verified:
                            confidence_diff = chosen_confidence - rejected_confidence
                            chosen_prob = 0.65 + 0.25 * confidence_diff
                            chosen_prob = max(0.55, min(0.85, chosen_prob))
                            chosen_prob_win = chosen_prob
                            chosen_prob_lose = 1 - chosen_prob
                        else:
                            if chosen_confidence > rejected_confidence:
                                chosen_prob = 0.58
                            else:
                                chosen_prob = 0.42
                            chosen_prob_win = chosen_prob
                            chosen_prob_lose = 1 - chosen_prob
                        
                        dataset_samples.append({
                            "prompt": question,
                            "chosen": chosen_response,
                            "rejected": rejected_response,
                            "chosen_probs": chosen_prob,
                            "chosen_probs_win": chosen_prob_win,
                            "chosen_probs_lose": chosen_prob_lose,
                            "chosen_verification": chosen_verification,
                            "rejected_verification": rejected_verification,
                            "v2_enhanced": True
                        })
        
        return dataset_samples
    
    # è¿è¡ŒV2å¢å¼ºç‰ˆå¼‚æ­¥æ•°æ®ç”Ÿæˆ
    samples = asyncio.run(generate_v2_verified_dataset())
    
    # åˆ›å»ºDatasetå¯¹è±¡
    dataset = Dataset.from_list(samples)
    
    print(f"âœ… åˆ›å»ºV2å¢å¼ºç‰ˆåˆæˆæ•°æ®é›†å®Œæˆï¼ŒåŒ…å« {len(samples)} ä¸ªéªŒè¯åå¥½å¯¹")
    return dataset