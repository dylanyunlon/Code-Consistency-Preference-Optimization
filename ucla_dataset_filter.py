#!/usr/bin/env python3
"""
UCLA Dataset Specialized Filter - ä¸“é—¨é’ˆå¯¹UCLA-AGIæ•°æ®é›†çš„è¿‡æ»¤å™¨
è¯†åˆ«é€‚åˆä»£ç éªŒè¯çš„æ•°å­¦/ç§‘å­¦é—®é¢˜ï¼Œè¿‡æ»¤æ‰LaTeXæ ¼å¼åŒ–ç­‰ä¸é€‚åˆçš„ä»»åŠ¡
"""

import re
import logging
from typing import List, Dict, Any, Tuple, Optional
import json

logger = logging.getLogger(__name__)

class UCLADatasetFilter:
    """UCLAæ•°æ®é›†ä¸“ç”¨è¿‡æ»¤å™¨"""
    
    def __init__(self):
        # æ•°å­¦/ç§‘å­¦è®¡ç®—å…³é”®è¯ï¼ˆé€‚åˆéªŒè¯ï¼‰
        self.math_science_keywords = [
            # æ•°å­¦è®¡ç®—
            'calculate', 'compute', 'solve', 'find', 'determine', 'evaluate',
            'sum', 'product', 'difference', 'quotient', 'equation', 'formula',
            'arithmetic', 'algebra', 'geometry', 'trigonometry', 'calculus',
            
            # æ•°é‡ç»Ÿè®¡
            'count', 'number', 'how many', 'total', 'average', 'mean', 'median',
            'frequency', 'percentage', 'ratio', 'proportion', 'probability',
            
            # ç‰©ç†/ç§‘å­¦
            'physics', 'chemistry', 'velocity', 'acceleration', 'force', 'energy',
            'mass', 'weight', 'distance', 'time', 'speed', 'temperature',
            'pressure', 'volume', 'density', 'concentration',
            
            # å‡ ä½•/æµ‹é‡
            'area', 'volume', 'perimeter', 'circumference', 'radius', 'diameter',
            'length', 'width', 'height', 'angle', 'degree', 'meter', 'cm', 'inch',
            
            # å­—ç¬¦ä¸²åˆ†æ
            'letter', 'character', 'word', 'occurrence', 'appears', 'contains',
            'string', 'text analysis', 'count letters', 'count words',
        ]
        
        # ä¸é€‚åˆéªŒè¯çš„ä»»åŠ¡ç±»å‹ï¼ˆä¸»è¦æ˜¯æ ¼å¼åŒ–ã€è§£é‡Šæ€§ä»»åŠ¡ï¼‰
        self.unsuitable_keywords = [
            # LaTeX/æ–‡æ¡£æ ¼å¼åŒ–
            'latex', 'documentclass', 'usepackage', 'begin{document}', 'end{document}',
            'table', 'tabular', 'caption', 'label', 'section', 'subsection',
            'bibliography', 'citation', 'format', 'structure', 'template',
            
            # ä»£ç æ ¼å¼åŒ–/é‡æ„
            'code structure', 'code format', 'refactor', 'optimize code',
            'programming', 'syntax', 'debug', 'compile', 'function definition',
            'class definition', 'variable declaration', 'import statement',
            
            # æ–‡æœ¬ç”Ÿæˆ/åˆ›ä½œ
            'write', 'create', 'generate', 'compose', 'draft', 'essay', 'article',
            'report', 'summary', 'description', 'explanation', 'analysis',
            'review', 'critique', 'discuss', 'compare', 'contrast',
            
            # è®¾è®¡æ¨¡å¼/æ¶æ„
            'design pattern', 'architecture', 'framework', 'methodology',
            'best practices', 'guidelines', 'principles', 'standards',
            
            # ä¸»è§‚åˆ¤æ–­/å»ºè®®
            'opinion', 'recommend', 'suggest', 'advice', 'preference',
            'better', 'worse', 'advantages', 'disadvantages', 'pros', 'cons',
            
            # è§£é‡Š/æ•™å­¦
            'explain', 'describe', 'what is', 'how to', 'why', 'definition',
            'concept', 'theory', 'principle', 'introduction', 'overview',
        ]
        
        # æ•°å­¦æ¨¡å¼è¯†åˆ«
        self.math_patterns = [
            r'\d+\s*[+\-*/Ã—Ã·]\s*\d+',  # åŸºæœ¬è¿ç®—
            r'\d+\s*=\s*\d+',          # ç­‰å¼
            r'\d+\s*%',                # ç™¾åˆ†æ¯”
            r'\d+\s*(cm|meter|inch|kg|gram|second|minute|hour)',  # å¸¦å•ä½
            r'x\s*=\s*\d+',            # å˜é‡èµ‹å€¼
            r'\d+\^\d+',               # æŒ‡æ•°
            r'sqrt\(\d+\)',            # å¹³æ–¹æ ¹
            r'\d+\s*factorial',        # é˜¶ä¹˜
            r'probability.*\d+',       # æ¦‚ç‡è®¡ç®—
        ]
        
        # é•¿åº¦å’Œå¤æ‚åº¦é˜ˆå€¼
        self.max_prompt_length = 300      # é™ä½æœ€å¤§é•¿åº¦
        self.max_candidate_length = 1000  # å€™é€‰å›ç­”æœ€å¤§é•¿åº¦
        self.max_latex_ratio = 0.3        # LaTeXå†…å®¹å æ¯”é˜ˆå€¼
    
    def analyze_prompt_content(self, prompt: str) -> Dict[str, Any]:
        """åˆ†æpromptå†…å®¹ç‰¹å¾"""
        prompt_lower = prompt.lower()
        
        analysis = {
            'length': len(prompt),
            'word_count': len(prompt.split()),
            'has_math_keywords': False,
            'has_unsuitable_keywords': False,
            'math_keyword_matches': [],
            'unsuitable_keyword_matches': [],
            'math_pattern_matches': [],
            'latex_ratio': 0.0,
            'is_question': False,
            'content_type': 'unknown'
        }
        
        # æ£€æŸ¥æ•°å­¦/ç§‘å­¦å…³é”®è¯
        for keyword in self.math_science_keywords:
            if keyword.lower() in prompt_lower:
                analysis['math_keyword_matches'].append(keyword)
                analysis['has_math_keywords'] = True
        
        # æ£€æŸ¥ä¸é€‚åˆçš„å…³é”®è¯
        for keyword in self.unsuitable_keywords:
            if keyword.lower() in prompt_lower:
                analysis['unsuitable_keyword_matches'].append(keyword)
                analysis['has_unsuitable_keywords'] = True
        
        # æ£€æŸ¥æ•°å­¦æ¨¡å¼
        for pattern in self.math_patterns:
            matches = re.findall(pattern, prompt, re.IGNORECASE)
            if matches:
                analysis['math_pattern_matches'].extend(matches)
        
        # è®¡ç®—LaTeXå†…å®¹å æ¯”
        latex_indicators = ['\\', '{', '}', 'begin{', 'end{', 'documentclass']
        latex_count = sum(prompt.count(indicator) for indicator in latex_indicators)
        analysis['latex_ratio'] = latex_count / max(len(prompt), 1)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é—®é¢˜å½¢å¼
        question_indicators = ['?', 'how', 'what', 'calculate', 'find', 'determine']
        analysis['is_question'] = any(indicator in prompt_lower for indicator in question_indicators)
        
        # ç¡®å®šå†…å®¹ç±»å‹
        if analysis['latex_ratio'] > self.max_latex_ratio:
            analysis['content_type'] = 'latex_formatting'
        elif analysis['has_unsuitable_keywords']:
            analysis['content_type'] = 'code_formatting_or_explanation'
        elif analysis['has_math_keywords'] or analysis['math_pattern_matches']:
            analysis['content_type'] = 'mathematical_or_scientific'
        elif analysis['is_question'] and analysis['length'] < self.max_prompt_length:
            analysis['content_type'] = 'potentially_suitable'
        else:
            analysis['content_type'] = 'unclear_or_unsuitable'
        
        return analysis
    
    def analyze_candidates(self, candidates: List[str]) -> Dict[str, Any]:
        """åˆ†æå€™é€‰å›ç­”çš„ç‰¹å¾"""
        if not candidates:
            return {'suitable': False, 'reason': 'No candidates provided'}
        
        analysis = {
            'candidate_count': len(candidates),
            'avg_length': 0,
            'max_length': 0,
            'latex_heavy_count': 0,
            'code_heavy_count': 0,
            'suitable_count': 0
        }
        
        total_length = 0
        for candidate in candidates:
            # ä»å€™é€‰å›ç­”ä¸­æå–å®é™…å†…å®¹
            if isinstance(candidate, str):
                # å¦‚æœæ˜¯JSONæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
                if candidate.strip().startswith('['):
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, list) and len(parsed) > 0:
                            # æå–assistantçš„å›ç­”
                            for item in parsed:
                                if isinstance(item, dict) and item.get('role') == 'assistant':
                                    content = item.get('content', '')
                                    break
                            else:
                                content = candidate
                        else:
                            content = candidate
                    except:
                        content = candidate
                else:
                    content = candidate
            else:
                content = str(candidate)
            
            length = len(content)
            total_length += length
            analysis['max_length'] = max(analysis['max_length'], length)
            
            # æ£€æŸ¥LaTeXå†…å®¹å æ¯”
            latex_count = content.count('\\') + content.count('begin{') + content.count('documentclass')
            if latex_count > 10:  # LaTeXå†…å®¹è¾ƒå¤š
                analysis['latex_heavy_count'] += 1
            
            # æ£€æŸ¥ä»£ç å†…å®¹
            code_indicators = ['def ', 'class ', 'import ', 'function', '#include', 'public class']
            if any(indicator in content for indicator in code_indicators):
                analysis['code_heavy_count'] += 1
            
            # æ£€æŸ¥æ˜¯å¦é€‚åˆéªŒè¯ï¼ˆé•¿åº¦åˆç†ä¸”éæ ¼å¼åŒ–å†…å®¹ï¼‰
            if length < self.max_candidate_length and latex_count < 5:
                analysis['suitable_count'] += 1
        
        analysis['avg_length'] = total_length / len(candidates)
        
        return analysis
    
    def is_suitable_for_verification(self, prompt: str, candidates: List[str]) -> Tuple[bool, str, Dict[str, Any]]:
        """
        åˆ¤æ–­æ ·æœ¬æ˜¯å¦é€‚åˆä»£ç éªŒè¯
        
        Returns:
            Tuple[bool, str, Dict]: (æ˜¯å¦é€‚åˆ, åŸå› , è¯¦ç»†åˆ†æ)
        """
        prompt_analysis = self.analyze_prompt_content(prompt)
        candidate_analysis = self.analyze_candidates(candidates)
        
        details = {
            'prompt_analysis': prompt_analysis,
            'candidate_analysis': candidate_analysis
        }
        
        # 1. æ£€æŸ¥prompté•¿åº¦
        if prompt_analysis['length'] > self.max_prompt_length:
            return False, f"Promptå¤ªé•¿ ({prompt_analysis['length']} > {self.max_prompt_length})", details
        
        # 2. æ£€æŸ¥å€™é€‰å›ç­”é•¿åº¦
        if candidate_analysis['avg_length'] > self.max_candidate_length:
            return False, f"å€™é€‰å›ç­”å¤ªé•¿ (å¹³å‡{candidate_analysis['avg_length']:.0f} > {self.max_candidate_length})", details
        
        # 3. æ£€æŸ¥LaTeXå†…å®¹å æ¯”
        if prompt_analysis['latex_ratio'] > self.max_latex_ratio:
            return False, f"LaTeXå†…å®¹è¿‡å¤š (å æ¯”{prompt_analysis['latex_ratio']:.2f} > {self.max_latex_ratio})", details
        
        # 4. æ£€æŸ¥æ˜¯å¦æ˜¯LaTeXæ ¼å¼åŒ–ä»»åŠ¡
        if candidate_analysis['latex_heavy_count'] >= len(candidates) * 0.5:
            return False, "ä¸»è¦æ˜¯LaTeXæ ¼å¼åŒ–ä»»åŠ¡", details
        
        # 5. æ£€æŸ¥ä¸é€‚åˆçš„å…³é”®è¯
        if prompt_analysis['has_unsuitable_keywords']:
            return False, f"åŒ…å«ä¸é€‚åˆå…³é”®è¯: {prompt_analysis['unsuitable_keyword_matches'][:3]}", details
        
        # 6. æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­¦/ç§‘å­¦å†…å®¹
        if prompt_analysis['has_math_keywords'] or prompt_analysis['math_pattern_matches']:
            return True, f"åŒ…å«æ•°å­¦/ç§‘å­¦å†…å®¹: {prompt_analysis['math_keyword_matches'][:3]}", details
        
        # 7. æ£€æŸ¥æ˜¯å¦æ˜¯ç®€çŸ­çš„é—®é¢˜
        if (prompt_analysis['is_question'] and 
            prompt_analysis['length'] < 200 and 
            candidate_analysis['suitable_count'] > 0):
            return True, "ç®€çŸ­é—®é¢˜ä¸”å€™é€‰å›ç­”é•¿åº¦åˆç†", details
        
        # 8. é»˜è®¤ä¸é€‚åˆ
        return False, f"å†…å®¹ç±»å‹ä¸æ˜ç¡®: {prompt_analysis['content_type']}", details

def filter_ucla_dataset(
    prompts: List[str],
    candidates_list: List[List[str]],
    max_samples: Optional[int] = None,
    debug: bool = False
) -> Tuple[List[str], List[List[str]], Dict[str, Any]]:
    """
    è¿‡æ»¤UCLAæ•°æ®é›†ï¼Œåªä¿ç•™é€‚åˆä»£ç éªŒè¯çš„æ ·æœ¬
    """
    filter_obj = UCLADatasetFilter()
    
    filtered_prompts = []
    filtered_candidates = []
    filter_stats = {
        'total_original': len(prompts),
        'suitable_count': 0,
        'unsuitable_count': 0,
        'filter_reasons': {},
        'content_types': {},
        'examples': {
            'suitable': [],
            'unsuitable': []
        }
    }
    
    print(f"ğŸ” å¼€å§‹è¿‡æ»¤UCLAæ•°æ®é›†ï¼ŒåŸå§‹æ ·æœ¬æ•°: {len(prompts)}")
    
    for i, (prompt, candidates) in enumerate(zip(prompts, candidates_list)):
        is_suitable, reason, details = filter_obj.is_suitable_for_verification(prompt, candidates)
        
        # ç»Ÿè®¡
        content_type = details['prompt_analysis']['content_type']
        if content_type not in filter_stats['content_types']:
            filter_stats['content_types'][content_type] = 0
        filter_stats['content_types'][content_type] += 1
        
        if reason not in filter_stats['filter_reasons']:
            filter_stats['filter_reasons'][reason] = 0
        filter_stats['filter_reasons'][reason] += 1
        
        if is_suitable:
            filtered_prompts.append(prompt)
            filtered_candidates.append(candidates)
            filter_stats['suitable_count'] += 1
            
            # ä¿å­˜é€‚åˆçš„ç¤ºä¾‹
            if len(filter_stats['examples']['suitable']) < 3:
                filter_stats['examples']['suitable'].append({
                    'index': i,
                    'prompt': prompt[:100] + "..." if len(prompt) > 100 else prompt,
                    'reason': reason,
                    'math_keywords': details['prompt_analysis']['math_keyword_matches'][:3]
                })
            
            if debug:
                print(f"âœ… æ ·æœ¬ {i}: é€‚åˆéªŒè¯")
                print(f"   é—®é¢˜: {prompt[:50]}...")
                print(f"   åŸå› : {reason}")
        else:
            filter_stats['unsuitable_count'] += 1
            
            # ä¿å­˜ä¸é€‚åˆçš„ç¤ºä¾‹
            if len(filter_stats['examples']['unsuitable']) < 3:
                filter_stats['examples']['unsuitable'].append({
                    'index': i,
                    'prompt': prompt[:100] + "..." if len(prompt) > 100 else prompt,
                    'reason': reason,
                    'content_type': content_type
                })
            
            if debug:
                print(f"âŒ æ ·æœ¬ {i}: ä¸é€‚åˆéªŒè¯")
                print(f"   é—®é¢˜: {prompt[:50]}...")
                print(f"   åŸå› : {reason}")
        
        # è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°æ—¶åœæ­¢
        if max_samples and filter_stats['suitable_count'] >= max_samples:
            print(f"âš ï¸  å·²è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶: {max_samples}")
            break
        
        # æ¯100ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (i + 1) % 100 == 0:
            print(f"ğŸ“Š è¿‡æ»¤è¿›åº¦: {i+1}/{len(prompts)}, é€‚åˆ: {filter_stats['suitable_count']}")
    
    return filtered_prompts, filtered_candidates, filter_stats

def print_ucla_filter_report(filter_stats: Dict[str, Any]):
    """æ‰“å°UCLAæ•°æ®é›†è¿‡æ»¤æŠ¥å‘Š"""
    print("\n" + "="*70)
    print("ğŸ“Š UCLAæ•°æ®é›†è¿‡æ»¤æŠ¥å‘Š")
    print("="*70)
    print(f"åŸå§‹æ ·æœ¬æ•°: {filter_stats['total_original']}")
    print(f"é€‚åˆéªŒè¯: {filter_stats['suitable_count']}")
    print(f"ä¸é€‚åˆéªŒè¯: {filter_stats['unsuitable_count']}")
    print(f"è¿‡æ»¤ç‡: {filter_stats['suitable_count']/filter_stats['total_original']*100:.1f}%")
    
    print(f"\nğŸ“‹ å†…å®¹ç±»å‹åˆ†å¸ƒ:")
    for content_type, count in filter_stats['content_types'].items():
        percentage = count / filter_stats['total_original'] * 100
        print(f"  {content_type}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ” ä¸»è¦è¿‡æ»¤åŸå› :")
    sorted_reasons = sorted(filter_stats['filter_reasons'].items(), key=lambda x: x[1], reverse=True)
    for reason, count in sorted_reasons[:5]:
        print(f"  {reason}: {count}")
    
    print(f"\nâœ… é€‚åˆéªŒè¯çš„ç¤ºä¾‹:")
    for example in filter_stats['examples']['suitable']:
        print(f"  - [{example['index']}] {example['prompt']}")
        print(f"    åŸå› : {example['reason']}")
        if example.get('math_keywords'):
            print(f"    æ•°å­¦å…³é”®è¯: {example['math_keywords']}")
    
    print(f"\nâŒ ä¸é€‚åˆéªŒè¯çš„ç¤ºä¾‹:")
    for example in filter_stats['examples']['unsuitable']:
        print(f"  - [{example['index']}] {example['prompt']}")
        print(f"    åŸå› : {example['reason']}")
        print(f"    ç±»å‹: {example['content_type']}")
    
    print("="*70)

# æµ‹è¯•å‡½æ•°
def test_ucla_filter():
    """æµ‹è¯•UCLAæ•°æ®é›†è¿‡æ»¤å™¨"""
    print("ğŸ§ª æµ‹è¯•UCLAæ•°æ®é›†è¿‡æ»¤å™¨")
    print("="*50)
    
    # æ¨¡æ‹ŸUCLAæ•°æ®é›†æ ·æœ¬
    test_samples = [
        # é€‚åˆçš„ï¼ˆæ•°å­¦è®¡ç®—ï¼‰
        {
            'prompt': 'Calculate the sum of numbers from 1 to 100',
            'candidates': ['The sum is 5050', 'Sum = 100*101/2 = 5050', '5050']
        },
        {
            'prompt': 'How many times does the letter "a" appear in "banana"?',
            'candidates': ['3 times', 'The letter a appears 3 times', 'Count: 3']
        },
        
        # ä¸é€‚åˆçš„ï¼ˆLaTeXæ ¼å¼åŒ–ï¼‰
        {
            'prompt': 'Please provide the content structure of the following text using [Latex] data format...',
            'candidates': [
                '[{"role": "assistant", "content": "\\\\documentclass{article}\\\\usepackage{booktabs}..."}]',
                '[{"role": "assistant", "content": "\\\\begin{document}\\\\maketitle..."}]'
            ]
        },
        
        # ä¸é€‚åˆçš„ï¼ˆä»£ç è§£é‡Šï¼‰
        {
            'prompt': 'Explain the design patterns used in this C# code',
            'candidates': ['This code uses Factory pattern...', 'The patterns include Singleton...']
        }
    ]
    
    prompts = [sample['prompt'] for sample in test_samples]
    candidates_list = [sample['candidates'] for sample in test_samples]
    
    filtered_prompts, filtered_candidates, stats = filter_ucla_dataset(
        prompts, candidates_list, debug=True
    )
    
    print_ucla_filter_report(stats)

if __name__ == "__main__":
    test_ucla_filter()