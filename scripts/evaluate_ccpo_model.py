#!/usr/bin/env python3
"""
CCPOæ¨¡å‹è¯„ä¼°è„šæœ¬
è¯„ä¼°åŸºäºä»£ç éªŒè¯è®­ç»ƒçš„æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°
"""

import asyncio
import json
import argparse
import logging
import time
from typing import Dict, List, Any, Tuple
from pathlib import Path
import numpy as np
from datasets import load_dataset, load_from_disk
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# å¯¼å…¥éªŒè¯å™¨
from execution_verifier import ExecutionVerifier

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="è¯„ä¼°CCPOè®­ç»ƒçš„æ¨¡å‹")
    parser.add_argument("--model_path", type=str, required=True,
                       help="CCPOè®­ç»ƒçš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_dataset", type=str, required=True,
                       help="æµ‹è¯•æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--verification_sample_size", type=int, default=50,
                       help="è¿›è¡Œä»£ç éªŒè¯çš„æ ·æœ¬æ•°é‡")
    parser.add_argument("--output_report", type=str, required=True,
                       help="è¯„ä¼°æŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    parser.add_argument("--max_new_tokens", type=int, default=512,
                       help="ç”Ÿæˆçš„æœ€å¤§tokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--top_p", type=float, default=0.9,
                       help="æ ¸é‡‡æ ·å‚æ•°")
    parser.add_argument("--verification_url", type=str, 
                       default="https://8.134.217.190:17432",
                       help="ä»£ç æ‰§è¡ŒéªŒè¯æœåŠ¡å™¨")
    parser.add_argument("--verification_username", type=str, default="newuser")
    parser.add_argument("--verification_password", type=str, default="newPass123")
    parser.add_argument("--debug", action="store_true")
    return parser.parse_args()


class CCPOModelEvaluator:
    """CCPOæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, args):
        self.args = args
        self.model = None
        self.tokenizer = None
        self.verification_stats = {
            'total_evaluated': 0,
            'verification_attempted': 0,
            'verification_successful': 0,
            'high_accuracy_responses': 0,
            'average_confidence': 0.0,
            'perfect_matches': 0,
            'generation_time': 0.0,
            'verification_time': 0.0
        }
    
    def load_model(self):
        """åŠ è½½CCPOè®­ç»ƒçš„æ¨¡å‹"""
        logger.info(f"ğŸ”„ åŠ è½½CCPOæ¨¡å‹: {self.args.model_path}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.args.model_path)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.args.model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            logger.info(f"   å‚æ•°é‡: {self.model.num_parameters():,}")
            logger.info(f"   è®¾å¤‡: {next(self.model.parameters()).device}")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_test_dataset(self):
        """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
        logger.info(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®é›†: {self.args.test_dataset}")
        
        try:
            if Path(self.args.test_dataset).exists():
                # æœ¬åœ°æ•°æ®é›†
                dataset = load_from_disk(self.args.test_dataset)
            else:
                # HuggingFaceæ•°æ®é›†
                dataset = load_dataset(self.args.test_dataset, split="test")
            
            logger.info(f"âœ… æµ‹è¯•æ•°æ®é›†åŠ è½½å®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")
            return dataset
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            raise
    
    def generate_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        formatted_prompt = self.tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}, {"role": "assistant", "content": ""}],
            tokenize=False,
            add_generation_prompt=True
        ).rstrip()
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to(self.model.device)
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.args.max_new_tokens,
                temperature=self.args.temperature,
                top_p=self.args.top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç å“åº”
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    async def evaluate_model(self) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info(f"ğŸ¯ å¼€å§‹CCPOæ¨¡å‹è¯„ä¼°")
        
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        self.load_model()
        test_dataset = self.load_test_dataset()
        
        # é€‰æ‹©è¯„ä¼°æ ·æœ¬
        eval_samples = min(self.args.verification_sample_size, len(test_dataset))
        eval_indices = np.random.choice(len(test_dataset), eval_samples, replace=False)
        
        logger.info(f"ğŸ“‹ è¯„ä¼°æ ·æœ¬æ•°: {eval_samples}")
        
        # ç”Ÿæˆå“åº”
        logger.info(f"ğŸ¯ å¼€å§‹ç”Ÿæˆå“åº”...")
        generation_start = time.time()
        
        generated_responses = []
        questions = []
        expected_answers = []
        
        for i, idx in enumerate(eval_indices):
            sample = test_dataset[int(idx)]
            question = sample['prompt']
            expected_answer = sample['expected_answer']
            
            try:
                response = self.generate_response(question)
                generated_responses.append(response)
                questions.append(question)
                expected_answers.append(expected_answer)
                
                if self.args.debug and i < 3:
                    logger.info(f"   æ ·æœ¬ {i+1}: {question[:50]}...")
                    logger.info(f"   å“åº”: {response[:100]}...")
                
            except Exception as e:
                logger.error(f"   âŒ ç”Ÿæˆå¤±è´¥ æ ·æœ¬ {i+1}: {e}")
                continue
        
        self.verification_stats['generation_time'] = time.time() - generation_start
        self.verification_stats['total_evaluated'] = len(generated_responses)
        
        logger.info(f"âœ… å“åº”ç”Ÿæˆå®Œæˆ: {len(generated_responses)} ä¸ª")
        
        # ä»£ç éªŒè¯è¯„ä¼°
        logger.info(f"ğŸ” å¼€å§‹ä»£ç éªŒè¯è¯„ä¼°...")
        verification_start = time.time()
        
        verification_results = await self._verify_responses(questions, generated_responses)
        
        self.verification_stats['verification_time'] = time.time() - verification_start
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        evaluation_metrics = self._calculate_metrics(
            questions, 
            generated_responses, 
            expected_answers, 
            verification_results
        )
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        report = self._generate_evaluation_report(evaluation_metrics)
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_report(report)
        
        return report
    
    async def _verify_responses(self, questions: List[str], responses: List[str]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ä»£ç éªŒè¯è¯„ä¼°å“åº”è´¨é‡"""
        verification_results = []
        
        try:
            async with ExecutionVerifier(
                base_url=self.args.verification_url,
                username=self.args.verification_username,
                password=self.args.verification_password,
                debug=self.args.debug
            ) as verifier:
                
                for i, (question, response) in enumerate(zip(questions, responses)):
                    self.verification_stats['verification_attempted'] += 1
                    
                    try:
                        # è¿›è¡Œä»£ç éªŒè¯
                        result = await verifier.verify_response(question, response)
                        
                        verification_info = {
                            'question': question,
                            'response': response,
                            'verified': result.verified,
                            'confidence': result.confidence,
                            'ai_answer': result.ai_answer,
                            'code_answer': result.code_answer,
                            'status': result.status.value,
                            'execution_time': result.execution_time,
                            'error_message': result.error_message
                        }
                        
                        verification_results.append(verification_info)
                        
                        if result.verified:
                            self.verification_stats['verification_successful'] += 1
                            if result.confidence > 0.9:
                                self.verification_stats['perfect_matches'] += 1
                        
                        if self.args.debug:
                            logger.info(f"   éªŒè¯ {i+1}: {'âœ…' if result.verified else 'âŒ'} "
                                      f"(ç½®ä¿¡åº¦: {result.confidence:.3f})")
                        
                        # é€‚å½“å»¶è¿Ÿä»¥é¿å…é™æµ
                        await asyncio.sleep(2)
                        
                    except Exception as e:
                        logger.error(f"   âŒ éªŒè¯å¤±è´¥ {i+1}: {e}")
                        verification_results.append({
                            'question': question,
                            'response': response,
                            'verified': False,
                            'confidence': 0.0,
                            'ai_answer': None,
                            'code_answer': None,
                            'status': 'error',
                            'execution_time': 0.0,
                            'error_message': str(e)
                        })
        
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        return verification_results
    
    def _calculate_metrics(
        self, 
        questions: List[str], 
        responses: List[str], 
        expected_answers: List[str], 
        verification_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # åŸºç¡€ç»Ÿè®¡
        total_samples = len(responses)
        verified_samples = sum(1 for r in verification_results if r['verified'])
        
        metrics['total_samples'] = total_samples
        metrics['verified_samples'] = verified_samples
        metrics['verification_rate'] = verified_samples / total_samples if total_samples > 0 else 0
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confidences = [r['confidence'] for r in verification_results]
        if confidences:
            metrics['average_confidence'] = np.mean(confidences)
            metrics['confidence_std'] = np.std(confidences)
            metrics['high_confidence_rate'] = sum(1 for c in confidences if c > 0.8) / len(confidences)
        
        # ç­”æ¡ˆåŒ¹é…åˆ†æ
        exact_matches = 0
        ai_answers = []
        code_answers = []
        
        for i, result in enumerate(verification_results):
            ai_answer = result.get('ai_answer')
            code_answer = result.get('code_answer')
            expected = expected_answers[i] if i < len(expected_answers) else None
            
            if ai_answer:
                ai_answers.append(ai_answer)
            if code_answer:
                code_answers.append(code_answer)
            
            # æ£€æŸ¥ä¸æœŸæœ›ç­”æ¡ˆçš„åŒ¹é…
            if expected and ai_answer:
                try:
                    if abs(float(ai_answer) - float(expected)) < 1e-6:
                        exact_matches += 1
                except (ValueError, TypeError):
                    if str(ai_answer).strip() == str(expected).strip():
                        exact_matches += 1
        
        metrics['exact_match_rate'] = exact_matches / total_samples if total_samples > 0 else 0
        metrics['ai_answer_extraction_rate'] = len(ai_answers) / total_samples if total_samples > 0 else 0
        metrics['code_answer_generation_rate'] = len(code_answers) / total_samples if total_samples > 0 else 0
        
        # æ€§èƒ½ç»Ÿè®¡
        metrics['average_generation_time'] = self.verification_stats['generation_time'] / total_samples if total_samples > 0 else 0
        metrics['average_verification_time'] = self.verification_stats['verification_time'] / len(verification_results) if verification_results else 0
        
        # é”™è¯¯åˆ†æ
        status_counts = {}
        for result in verification_results:
            status = result.get('status', 'unknown')
            status_counts[status] = status_counts.get(status, 0) + 1
        metrics['status_distribution'] = status_counts
        
        return metrics
    
    def _generate_evaluation_report(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = {
            'model_path': self.args.model_path,
            'test_dataset': self.args.test_dataset,
            'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'evaluation_settings': {
                'verification_sample_size': self.args.verification_sample_size,
                'max_new_tokens': self.args.max_new_tokens,
                'temperature': self.args.temperature,
                'top_p': self.args.top_p
            },
            'metrics': metrics,
            'summary': self._generate_summary(metrics)
        }
        
        return report
    
    def _generate_summary(self, metrics: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆè¯„ä¼°æ‘˜è¦"""
        verification_rate = metrics.get('verification_rate', 0)
        exact_match_rate = metrics.get('exact_match_rate', 0)
        average_confidence = metrics.get('average_confidence', 0)
        
        # è¯„ä¼°ç­‰çº§
        if verification_rate > 0.8 and exact_match_rate > 0.7:
            grade = "A (ä¼˜ç§€)"
        elif verification_rate > 0.6 and exact_match_rate > 0.5:
            grade = "B (è‰¯å¥½)"
        elif verification_rate > 0.4 and exact_match_rate > 0.3:
            grade = "C (ä¸€èˆ¬)"
        else:
            grade = "D (éœ€è¦æ”¹è¿›)"
        
        summary = {
            'overall_grade': grade,
            'verification_performance': f"{verification_rate:.1%}",
            'accuracy_performance': f"{exact_match_rate:.1%}",
            'confidence_level': f"{average_confidence:.3f}",
            'recommendation': self._get_recommendation(verification_rate, exact_match_rate, average_confidence)
        }
        
        return summary
    
    def _get_recommendation(self, verification_rate: float, exact_match_rate: float, confidence: float) -> str:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        if verification_rate < 0.3:
            return "å»ºè®®å¢åŠ ä»£ç éªŒè¯ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œæé«˜æ¨¡å‹ç”Ÿæˆå¯éªŒè¯å›ç­”çš„èƒ½åŠ›"
        elif exact_match_rate < 0.4:
            return "å»ºè®®ä¼˜åŒ–ç­”æ¡ˆæå–å’Œæ ¼å¼åŒ–ï¼Œæé«˜ç­”æ¡ˆå‡†ç¡®æ€§"
        elif confidence < 0.6:
            return "å»ºè®®è°ƒæ•´è®­ç»ƒå‚æ•°ï¼Œæé«˜æ¨¡å‹å›ç­”çš„ç½®ä¿¡åº¦"
        else:
            return "æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå¯è€ƒè™‘åœ¨æ›´å¤æ‚çš„æ•°å­¦é—®é¢˜ä¸Šè¿›è¡Œæµ‹è¯•"
    
    def _save_report(self, report: Dict[str, Any]):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        output_path = Path(self.args.output_report)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(report)
    
    def _print_summary(self, report: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        metrics = report['metrics']
        summary = report['summary']
        
        print(f"\n" + "="*70)
        print(f"ğŸ“Š CCPOæ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        print(f"="*70)
        print(f"æ¨¡å‹è·¯å¾„: {report['model_path']}")
        print(f"è¯„ä¼°æ—¶é—´: {report['evaluation_timestamp']}")
        print(f"æ ·æœ¬æ•°é‡: {metrics['total_samples']}")
        
        print(f"\nğŸ¯ æ ¸å¿ƒæŒ‡æ ‡:")
        print(f"   ä»£ç éªŒè¯æˆåŠŸç‡: {summary['verification_performance']}")
        print(f"   ç­”æ¡ˆå‡†ç¡®ç‡: {summary['accuracy_performance']}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {summary['confidence_level']}")
        print(f"   ç»¼åˆè¯„çº§: {summary['overall_grade']}")
        
        print(f"\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡:")
        print(f"   éªŒè¯æ ·æœ¬æ•°: {metrics['verified_samples']}/{metrics['total_samples']}")
        print(f"   é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {metrics.get('high_confidence_rate', 0):.1%}")
        print(f"   AIç­”æ¡ˆæå–ç‡: {metrics.get('ai_answer_extraction_rate', 0):.1%}")
        print(f"   ä»£ç ç­”æ¡ˆç”Ÿæˆç‡: {metrics.get('code_answer_generation_rate', 0):.1%}")
        
        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {metrics.get('average_generation_time', 0):.2f}ç§’/æ ·æœ¬")
        print(f"   å¹³å‡éªŒè¯æ—¶é—´: {metrics.get('average_verification_time', 0):.2f}ç§’/æ ·æœ¬")
        
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        print(f"   {summary['recommendation']}")
        
        print(f"="*70)


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    logger.info(f"ğŸš€ CCPOæ¨¡å‹è¯„ä¼°å™¨å¯åŠ¨")
    logger.info(f"æ¨¡å‹: {args.model_path}")
    logger.info(f"æµ‹è¯•é›†: {args.test_dataset}")
    logger.info(f"éªŒè¯æ ·æœ¬æ•°: {args.verification_sample_size}")
    
    evaluator = CCPOModelEvaluator(args)
    
    try:
        report = await evaluator.evaluate_model()
        logger.info(f"âœ… è¯„ä¼°å®Œæˆ!")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))