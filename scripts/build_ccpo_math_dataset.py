#!/usr/bin/env python3
"""
CCPOæ•°å­¦æ•°æ®é›†æ„å»ºå™¨ - ç®€åŒ–ç‰ˆ
ç›´æ¥ä½¿ç”¨MetaMathçš„responseå’ŒOlympiadBenchçš„final_answer
"""

import json
import argparse
import logging
import random
from typing import Dict, List, Any
from pathlib import Path
from datasets import load_dataset, Dataset
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="æ„å»ºCCPOæ•°å­¦æ•°æ®é›† - ç®€åŒ–ç‰ˆ")
    parser.add_argument("--output_path", type=str, default="/data/jiacheng/dylan/iclr2026/ccpo_math_dataset")
    parser.add_argument("--target_size", type=int, default=60000)
    parser.add_argument("--test_split_ratio", type=float, default=0.1)
    parser.add_argument("--push_to_hub", action="store_true")
    parser.add_argument("--hub_dataset_id", type=str, default="UCLA-AGI/ccpo-math-60k")
    parser.add_argument("--debug", action="store_true")
    parser.add_argument("--verify_samples", type=int, default=10)
    parser.add_argument("--include_olympiad", action="store_true", 
                       help="åŒ…å«OlympiadBenchç­‰ç«èµ›é¢˜")
    return parser.parse_args()


class SimpleCCPODatasetBuilder:
    """ç®€åŒ–çš„CCPOæ•°å­¦æ•°æ®é›†æ„å»ºå™¨ - ç›´æ¥ä½¿ç”¨ç°æˆçš„answerå­—æ®µ"""
    
    def __init__(self, args):
        self.args = args
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.processing_stats = {
            'total_processed': 0,
            'metamath_processed': 0,
            'olympiad_processed': 0,
            'final_count': 0
        }
    
    def build_dataset(self, target_size: int = 60000) -> Dataset:
        """æ„å»ºCCPOæ•°å­¦æ•°æ®é›†"""
        logger.info(f"ğŸš€ å¼€å§‹æ„å»ºCCPOæ•°å­¦æ•°æ®é›†ï¼ˆç®€åŒ–ç‰ˆï¼‰")
        logger.info(f"   ç›®æ ‡å¤§å°: {target_size:,}")
        logger.info(f"   åŒ…å«ç«èµ›é¢˜: {self.args.include_olympiad}")
        
        all_problems = []
        
        # å®šä¹‰æ•°æ®æºé…ç½®
        data_sources = self._get_data_source_config(target_size)
        
        for source_config in data_sources:
            logger.info(f"\nğŸ“Š å¤„ç†æ•°æ®æº: {source_config['name']}")
            logger.info(f"   ç›®æ ‡æ•°é‡: {source_config['target']:,}")
            
            try:
                problems = source_config['processor'](source_config['target'])
                all_problems.extend(problems)
                logger.info(f"   âœ… è·å¾—é—®é¢˜: {len(problems):,}")
                
            except Exception as e:
                logger.error(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                if self.args.debug:
                    import traceback
                    traceback.print_exc()
        
        # éšæœºæ‰“ä¹±å¹¶æˆªå–ç›®æ ‡å¤§å°
        random.shuffle(all_problems)
        final_problems = all_problems[:target_size]
        self.processing_stats['final_count'] = len(final_problems)
        
        # è½¬æ¢ä¸ºDataset
        dataset = self._convert_to_dataset(final_problems)
        
        # è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š
        self._print_final_stats()
        
        return dataset
    
    def _get_data_source_config(self, target_size: int) -> List[Dict[str, Any]]:
        """è·å–æ•°æ®æºé…ç½®"""
        configs = [
            {
                'name': 'metamath',
                'target': int(target_size * 0.7),  # 70%
                'processor': self._process_metamath,
            }
        ]
        
        if self.args.include_olympiad:
            configs.insert(0, {
                'name': 'olympiad_bench',
                'target': int(target_size * 0.3),  # 30%
                'processor': self._process_olympiad_bench,
            })
            # è°ƒæ•´MetaMathæ¯”ä¾‹
            configs[1]['target'] = int(target_size * 0.7)
        
        return configs
    
    def _process_metamath(self, target_count: int) -> List[Dict[str, Any]]:
        """å¤„ç†MetaMathæ•°æ®é›† - ç›´æ¥ä½¿ç”¨responseå­—æ®µ"""
        logger.info(f"   ğŸ”¬ åŠ è½½MetaMathæ•°æ®é›†...")
        
        try:
            dataset = load_dataset("meta-math/MetaMathQA-40K", split="train")
            logger.info(f"   âœ… MetaMathåŠ è½½æˆåŠŸ: {len(dataset)} æ¡è®°å½•")
            
            if self.args.debug:
                dataset = dataset.select(range(min(2000, len(dataset))))
                logger.info(f"   ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼Œé™åˆ¶ä¸º: {len(dataset)} æ¡è®°å½•")
            
            problems = []
            
            for item in dataset:
                if len(problems) >= target_count:
                    break
                
                self.processing_stats['total_processed'] += 1
                
                question = item['query']
                response = item['response']  # ç›´æ¥ä½¿ç”¨responseä½œä¸ºanswer
                
                # åŸºæœ¬æœ‰æ•ˆæ€§æ£€æŸ¥
                if not question or not response or len(question) < 10:
                    continue
                
                problems.append({
                    'question': question,
                    'answer': response,  # ç›´æ¥ä½¿ç”¨å®Œæ•´çš„response
                    'source': 'metamath'
                })
                
                self.processing_stats['metamath_processed'] += 1
                
                # è¿›åº¦æŠ¥å‘Š
                if len(problems) % 1000 == 0:
                    logger.info(f"     MetaMathå¤„ç†è¿›åº¦: {len(problems)}")
            
            logger.info(f"   âœ… MetaMathå¤„ç†å®Œæˆ: {len(problems)} ä¸ªé—®é¢˜")
            return problems
            
        except Exception as e:
            logger.error(f"   âŒ MetaMathå¤„ç†å¤±è´¥: {e}")
            if self.args.debug:
                import traceback
                traceback.print_exc()
            return []
    
    def _process_olympiad_bench(self, target_count: int) -> List[Dict[str, Any]]:
        """å¤„ç†OlympiadBenchæ•°æ®é›† - ç›´æ¥ä½¿ç”¨final_answerå­—æ®µ"""
        logger.info(f"   ğŸ† å¤„ç†OlympiadBenchç«èµ›é¢˜...")
        
        try:
            dataset = load_dataset("lmms-lab/OlympiadBench", split="test_en")
            if self.args.debug:
                dataset = dataset.select(range(min(500, len(dataset))))
            
            logger.info(f"   âœ… OlympiadBenchåŠ è½½æˆåŠŸ: {len(dataset)} æ¡è®°å½•")
            
            problems = []
            
            for item in dataset:
                if len(problems) >= target_count:
                    break
                
                self.processing_stats['total_processed'] += 1
                
                # ä½¿ç”¨questionå­—æ®µä½œä¸ºé—®é¢˜
                question = item.get('question', '')
                
                # ç›´æ¥ä½¿ç”¨final_answerå­—æ®µ
                final_answers = item.get('final_answer', [])
                if not final_answers or len(final_answers) == 0:
                    continue
                
                # å°†final_answeråˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                if isinstance(final_answers, list):
                    answer = str(final_answers[0]) if final_answers else ""
                else:
                    answer = str(final_answers)
                
                # åŸºæœ¬æœ‰æ•ˆæ€§æ£€æŸ¥
                if not question or not answer or len(question) < 10:
                    continue
                
                problems.append({
                    'question': question,
                    'answer': answer,  # ç›´æ¥ä½¿ç”¨final_answer
                    'source': 'olympiad_bench'
                })
                
                self.processing_stats['olympiad_processed'] += 1
                
                # è¿›åº¦æŠ¥å‘Š
                if len(problems) % 100 == 0:
                    logger.info(f"     OlympiadBenchå¤„ç†è¿›åº¦: {len(problems)}")
            
            logger.info(f"   âœ… OlympiadBenchå¤„ç†å®Œæˆ: {len(problems)} ä¸ªé—®é¢˜")
            return problems
            
        except Exception as e:
            logger.error(f"   âŒ OlympiadBenchå¤„ç†å¤±è´¥: {e}")
            if self.args.debug:
                import traceback
                traceback.print_exc()
            return []
    
    def _convert_to_dataset(self, problems: List[Dict[str, Any]]) -> Dataset:
        """è½¬æ¢ä¸ºDatasetæ ¼å¼"""
        if not problems:
            raise ValueError("æ²¡æœ‰é—®é¢˜å¯è½¬æ¢ä¸ºæ•°æ®é›†")
        
        dataset_dict = {
            'prompt': [p['question'] for p in problems],
            'answer': [p['answer'] for p in problems],  # ç»Ÿä¸€çš„answerå­—æ®µ
            'source': [p['source'] for p in problems],
        }
        
        return Dataset.from_dict(dataset_dict)
    
    def _save_as_huggingface_format(self, dataset: Dataset, output_path: str):
        """ä¿å­˜ä¸ºHuggingFaceå…¼å®¹æ ¼å¼"""
        logger.info(f"ğŸ’¾ ä¿å­˜ä¸ºHuggingFaceæ ¼å¼: {output_path}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºArrowæ ¼å¼çš„parquetæ–‡ä»¶
        dataset_file = output_dir / "dataset.parquet"
        dataset.to_parquet(dataset_file)
        
        # åˆ›å»ºdataset_info.json
        dataset_info = {
            "citation": "",
            "description": "CCPO Math Dataset - MetaMath + OlympiadBench",
            "features": {
                "prompt": {"dtype": "string", "_type": "Value"},
                "answer": {"dtype": "string", "_type": "Value"},
                "source": {"dtype": "string", "_type": "Value"},
            },
            "homepage": "",
            "license": "",
            "size_in_bytes": dataset_file.stat().st_size,
            "splits": {
                "train": {
                    "name": "train",
                    "num_bytes": dataset_file.stat().st_size,
                    "num_examples": len(dataset),
                    "shard_lengths": [len(dataset)],
                    "dataset_name": "ccpo_math_dataset"
                }
            },
            "version": {"version_str": "1.0.0", "major": 1, "minor": 0, "patch": 0}
        }
        
        with open(output_dir / "dataset_info.json", 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        # åˆ›å»ºstate.json
        state_info = {
            "_data_files": [{"filename": "dataset.parquet"}],
            "_fingerprint": "ccpo_math_dataset_v1",
            "_format_columns": None,
            "_format_kwargs": {},
            "_format_type": None,
            "_output_all_columns": False,
            "_split": "train"
        }
        
        with open(output_dir / "state.json", 'w') as f:
            json.dump(state_info, f, indent=2)
        
        logger.info(f"âœ… HuggingFaceæ ¼å¼ä¿å­˜å®Œæˆ")
        logger.info(f"   æ•°æ®æ–‡ä»¶: {dataset_file}")
        logger.info(f"   å…ƒæ•°æ®: dataset_info.json, state.json")
        
        return output_dir
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        stats = self.processing_stats
        
        print(f"\n" + "="*80)
        print(f"ğŸ“Š CCPOæ•°å­¦æ•°æ®é›†æ„å»ºå®Œæˆ (ç®€åŒ–ç‰ˆ)")
        print(f"="*80)
        print(f"ğŸ¯ æœ€ç»ˆæ•°æ®é›†å¤§å°: {stats['final_count']:,}")
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»å¤„ç†æ ·æœ¬: {stats['total_processed']:,}")
        print(f"   MetaMathå¤„ç†: {stats['metamath_processed']:,}")
        print(f"   OlympiadBenchå¤„ç†: {stats['olympiad_processed']:,}")
        
        print(f"\nâœ… æ•°æ®æºç‰¹è‰²:")
        print(f"   âœ“ MetaMath: ç›´æ¥ä½¿ç”¨responseå­—æ®µä½œä¸ºanswer")
        print(f"   âœ“ OlympiadBench: ç›´æ¥ä½¿ç”¨final_answerå­—æ®µä½œä¸ºanswer")
        print(f"   âœ“ é€‚åˆCCPOè®­ç»ƒçš„ç»Ÿä¸€answeræ ¼å¼")
        print(f"   âœ“ æ— éœ€å¤æ‚çš„ç­”æ¡ˆæå–é€»è¾‘")
        print(f"="*80)


def main():
    """ä¸»å‡½æ•°"""
    import time
    
    args = parse_arguments()
    
    print(f"ğŸš€ CCPOæ•°å­¦æ•°æ®é›†æ„å»ºå™¨ (ç®€åŒ–ç‰ˆ)")
    print(f"="*60)
    print(f"ç›®æ ‡å¤§å°: {args.target_size:,}")
    print(f"åŒ…å«ç«èµ›é¢˜: {args.include_olympiad}")
    print(f"è¾“å‡ºè·¯å¾„: {args.output_path}")
    
    # æ„å»ºæ•°æ®é›†
    builder = SimpleCCPODatasetBuilder(args)
    start_time = time.time()
    
    try:
        dataset = builder.build_dataset(target_size=args.target_size)
        
        build_time = time.time() - start_time
        print(f"\nâ±ï¸  æ€»æ„å»ºæ—¶é—´: {build_time:.1f}ç§’")
        
        # ä¿å­˜ä¸ºHuggingFaceå…¼å®¹æ ¼å¼
        logger.info(f"ğŸ’¾ ä¿å­˜æ•°æ®é›†...")
        saved_path = builder._save_as_huggingface_format(dataset, args.output_path)
        
        # åˆ›å»ºåˆ†å‰²
        if args.test_split_ratio > 0:
            logger.info(f"ğŸ”„ åˆ›å»ºè®­ç»ƒ/æµ‹è¯•åˆ†å‰²...")
            split_dataset = dataset.train_test_split(test_size=args.test_split_ratio, seed=42)
            
            # åˆ†åˆ«ä¿å­˜è®­ç»ƒå’Œæµ‹è¯•é›†
            train_path = f"{args.output_path}_train"
            test_path = f"{args.output_path}_test"
            
            builder._save_as_huggingface_format(split_dataset['train'], train_path)
            builder._save_as_huggingface_format(split_dataset['test'], test_path)
            
            logger.info(f"âœ… åˆ†å‰²ä¿å­˜å®Œæˆ:")
            logger.info(f"   è®­ç»ƒé›†: {train_path} ({len(split_dataset['train'])} æ ·æœ¬)")
            logger.info(f"   æµ‹è¯•é›†: {test_path} ({len(split_dataset['test'])} æ ·æœ¬)")
        
        # æ¨é€åˆ°Hubï¼ˆå¯é€‰ï¼‰
        if args.push_to_hub:
            logger.info(f"ğŸ”„ æ¨é€åˆ°HuggingFace Hub...")
            try:
                dataset.push_to_hub(args.hub_dataset_id)
                logger.info(f"âœ… æˆåŠŸæ¨é€: {args.hub_dataset_id}")
            except Exception as e:
                logger.error(f"âŒ æ¨é€å¤±è´¥: {e}")
        
        # éªŒè¯å¯ä»¥ç”¨load_datasetåŠ è½½
        logger.info(f"ğŸ” éªŒè¯æ•°æ®é›†å¯åŠ è½½æ€§...")
        try:
            from datasets import load_dataset
            test_dataset = load_dataset(str(saved_path), split="train")
            logger.info(f"âœ… éªŒè¯æˆåŠŸ: å¯ä»¥ä½¿ç”¨ load_dataset('{saved_path}', split='train') åŠ è½½")
            logger.info(f"   åŠ è½½çš„æ•°æ®é›†å¤§å°: {len(test_dataset)}")
            logger.info(f"   åˆ—å: {test_dataset.column_names}")
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            logger.info(f"å›é€€æ–¹æ¡ˆ: ä½¿ç”¨ load_from_disk('{saved_path}') åŠ è½½")
        
        # éªŒè¯æ ·æœ¬
        if args.verify_samples > 0:
            print(f"\nğŸ” è´¨é‡éªŒè¯æ ·æœ¬:")
            samples = dataset.select(range(min(args.verify_samples, len(dataset))))
            
            for i, sample in enumerate(samples):
                print(f"\næ ·æœ¬ {i+1}:")
                print(f"  é—®é¢˜: {sample['prompt'][:120]}...")
                print(f"  ç­”æ¡ˆ: {sample['answer'][:120]}...")  # æ˜¾ç¤ºanswerå­—æ®µå‰120ä¸ªå­—ç¬¦
                print(f"  æ¥æº: {sample['source']}")
        
        print(f"\nğŸ‰ CCPOæ•°å­¦æ•°æ®é›†æ„å»ºæˆåŠŸ! (ç®€åŒ–ç‰ˆ)")
        print(f"æ•°æ®é›†ä½ç½®: {saved_path}")
        print(f"æ€»é—®é¢˜æ•°: {len(dataset):,}")
        print(f"\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
        print(f"from datasets import load_dataset")
        print(f"dataset = load_dataset('{saved_path}', split='train')")
        print(f"\næ•°æ®æ ¼å¼:")
        print(f"- prompt: é—®é¢˜æ–‡æœ¬")
        print(f"- answer: MetaMathçš„responseæˆ–OlympiadBenchçš„final_answer")
        print(f"- source: æ•°æ®æ¥æº (metamath/olympiad_bench)")
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ æ„å»ºå¤±è´¥: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())