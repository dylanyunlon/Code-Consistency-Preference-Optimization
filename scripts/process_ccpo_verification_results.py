#!/usr/bin/env python3
"""
CCPOéªŒè¯ç»“æœå¤„ç†è„šæœ¬ - å¯¹è¯æ ¼å¼ç‰ˆæœ¬
å°†code_verified_rank.pyçš„éªŒè¯ç»“æœè½¬æ¢ä¸ºCCPOè®­ç»ƒéœ€è¦çš„å¯¹è¯æ ¼å¼æ•°æ®
"""

import json
import numpy as np
import pandas as pd
import argparse
import os
from typing import Dict, List, Tuple, Any
from datasets import Dataset, DatasetDict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="å¤„ç†CCPOéªŒè¯ç»“æœ")
    parser.add_argument("--input_dir", type=str, required=True, help="è¾“å…¥ç›®å½•ï¼ˆgeneratedæ•°æ®ï¼‰")
    parser.add_argument("--ranking_dir", type=str, required=True, help="æ’åç»“æœç›®å½•")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--data_frac", type=int, default=0, help="æ•°æ®åˆ†ç‰‡ç¼–å·")
    parser.add_argument("--pairs", type=int, default=5, help="æ¯ä¸ªé—®é¢˜çš„æ¨ç†è¿‡ç¨‹æ•°")
    parser.add_argument("--score_threshold", type=float, default=5.0, help="CCPOåˆ†æ•°å·®å¼‚é˜ˆå€¼")
    parser.add_argument("--confidence_threshold", type=float, default=0.1, help="ç½®ä¿¡åº¦å·®å¼‚é˜ˆå€¼")
    parser.add_argument("--output_format", type=str, default="conversation", 
                      choices=["conversation", "string"], help="è¾“å‡ºæ ¼å¼ï¼šconversationï¼ˆå¯¹è¯æ ¼å¼ï¼‰æˆ–stringï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰")
    return parser.parse_args()

class CCPOResultProcessor:
    """CCPOéªŒè¯ç»“æœå¤„ç†å™¨ - æ”¯æŒå¯¹è¯æ ¼å¼è¾“å‡º"""
    
    def __init__(self, args):
        self.args = args
        self.stats = {
            'total_samples': 0,
            'high_quality_pairs': 0,
            'low_quality_pairs': 0,
            'skipped_samples': 0,
            'avg_score_difference': 0.0
        }
    
    def load_data(self) -> Tuple[List[Dict], List[str], List[List[str]], np.ndarray]:
        """åŠ è½½ç”Ÿæˆçš„æ•°æ®å’ŒCCPOéªŒè¯ç»“æœ"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®...")
        
        # 1. åŠ è½½å…ƒæ•°æ®
        metadata_file = f"{self.args.input_dir}/metadata_{self.args.data_frac}.json"
        if not os.path.exists(metadata_file):
            raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        data_items = metadata['data_items']
        questions = [item['prompt'] for item in data_items]
        ground_truths = [item['answer'] for item in data_items]
        
        print(f"âœ… åŠ è½½å…ƒæ•°æ®: {len(data_items)} æ¡")
        
        # 2. åŠ è½½ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹
        all_responses = []
        for i in range(self.args.pairs):
            response_file = f"{self.args.input_dir}/responses_{i}.json"
            if not os.path.exists(response_file):
                raise FileNotFoundError(f"å“åº”æ–‡ä»¶ä¸å­˜åœ¨: {response_file}")
            
            with open(response_file, 'r', encoding='utf-8') as f:
                responses = json.load(f)
                all_responses.append(responses)
        
        # è½¬ç½®ä¸ºæ¯ä¸ªé—®é¢˜çš„æ¨ç†è¿‡ç¨‹åˆ—è¡¨
        reasoning_processes = list(zip(*all_responses))
        print(f"âœ… åŠ è½½æ¨ç†è¿‡ç¨‹: {len(reasoning_processes)} ä¸ªé—®é¢˜ï¼Œæ¯ä¸ª {self.args.pairs} ä¸ªæ¨ç†è¿‡ç¨‹")
        
        # 3. åŠ è½½CCPOéªŒè¯ç»“æœ
        ranking_file = f"{self.args.ranking_dir}/ccpo_0_{self.args.data_frac}.npy"
        if not os.path.exists(ranking_file):
            raise FileNotFoundError(f"CCPOæ’åæ–‡ä»¶ä¸å­˜åœ¨: {ranking_file}")
        
        ccpo_scores = np.load(ranking_file)
        print(f"âœ… åŠ è½½CCPOåˆ†æ•°: {ccpo_scores.shape}")
        
        return data_items, questions, reasoning_processes, ccpo_scores
    
    def create_preference_pairs(
        self, 
        questions: List[str], 
        reasoning_processes: List[List[str]], 
        ccpo_scores: np.ndarray
    ) -> List[Dict[str, Any]]:
        """åŸºäºCCPOåˆ†æ•°åˆ›å»ºåå¥½å¯¹"""
        print(f"ğŸ”„ åˆ›å»ºåå¥½å¯¹...")
        print(f"   è¾“å‡ºæ ¼å¼: {self.args.output_format}")
        
        preference_pairs = []
        
        for idx, (question, processes, scores) in enumerate(zip(questions, reasoning_processes, ccpo_scores)):
            self.stats['total_samples'] += 1
            
            # æ‰¾åˆ°æœ€é«˜åˆ†å’Œæœ€ä½åˆ†çš„æ¨ç†è¿‡ç¨‹
            max_idx = np.argmax(scores)
            min_idx = np.argmin(scores)
            
            max_score = scores[max_idx]
            min_score = scores[min_idx]
            score_diff = max_score - min_score
            
            # æ£€æŸ¥åˆ†æ•°å·®å¼‚æ˜¯å¦è¶³å¤Ÿå¤§
            if score_diff < self.args.score_threshold:
                self.stats['skipped_samples'] += 1
                continue
            
            chosen_process = processes[max_idx]
            rejected_process = processes[min_idx]
            
            # è®¡ç®—é€‰æ‹©æ¦‚ç‡ï¼ˆåŸºäºCCPOåˆ†æ•°ï¼‰
            chosen_prob = self._calculate_preference_probability(max_score, min_score)
            
            if self.args.output_format == "conversation":
                # å¯¹è¯æ ¼å¼ï¼šæ¯ä¸ªéƒ½æ˜¯æ¶ˆæ¯åˆ—è¡¨
                preference_pair = {
                    'chosen': [
                        {"role": "user", "content": question},
                        {"role": "assistant", "content": chosen_process}
                    ],
                    'rejected': [
                        {"role": "user", "content": question},
                        {"role": "assistant", "content": rejected_process}
                    ],
                    'chosen_probs': chosen_prob,
                    'chosen_probs_win': chosen_prob,
                    'chosen_probs_lose': 1.0 - chosen_prob,
                    'chosen_score': float(max_score),
                    'rejected_score': float(min_score),
                    'score_difference': float(score_diff),
                    'chosen_verification_score': self._normalize_score(max_score),
                    'rejected_verification_score': self._normalize_score(min_score),
                    'original_index': idx,
                    'ccpo_quality': 'high' if score_diff > 10.0 else 'medium'
                }
            else:
                # å­—ç¬¦ä¸²æ ¼å¼ï¼ˆåŸæ¥çš„æ ¼å¼ï¼‰
                preference_pair = {
                    'prompt': question,
                    'chosen': chosen_process,
                    'rejected': rejected_process,
                    'chosen_probs': chosen_prob,
                    'chosen_probs_win': chosen_prob,
                    'chosen_probs_lose': 1.0 - chosen_prob,
                    'chosen_score': float(max_score),
                    'rejected_score': float(min_score),
                    'score_difference': float(score_diff),
                    'chosen_verification_score': self._normalize_score(max_score),
                    'rejected_verification_score': self._normalize_score(min_score),
                    'original_index': idx,
                    'ccpo_quality': 'high' if score_diff > 10.0 else 'medium'
                }
            
            preference_pairs.append(preference_pair)
            
            if max_score > 0:
                self.stats['high_quality_pairs'] += 1
            else:
                self.stats['low_quality_pairs'] += 1
            
            self.stats['avg_score_difference'] += score_diff
        
        if preference_pairs:
            self.stats['avg_score_difference'] /= len(preference_pairs)
        
        print(f"âœ… åˆ›å»ºåå¥½å¯¹å®Œæˆ: {len(preference_pairs)} å¯¹")
        return preference_pairs
    
    def _calculate_preference_probability(self, chosen_score: float, rejected_score: float) -> float:
        """è®¡ç®—é€‰æ‹©æ¦‚ç‡"""
        # ä½¿ç”¨sigmoidå‡½æ•°å°†åˆ†æ•°å·®å¼‚è½¬æ¢ä¸ºæ¦‚ç‡
        score_diff = chosen_score - rejected_score
        
        # ç¡®ä¿æ¦‚ç‡åœ¨åˆç†èŒƒå›´å†…
        if score_diff > 20:
            return 0.95
        elif score_diff > 10:
            return 0.85
        elif score_diff > 5:
            return 0.75
        elif score_diff > 0:
            return 0.65
        else:
            return 0.55  # å³ä½¿åˆ†æ•°ç›¸è¿‘ï¼Œä¹Ÿè¦æœ‰è½»å¾®åå¥½
    
    def _normalize_score(self, score: float) -> float:
        """å½’ä¸€åŒ–åˆ†æ•°åˆ°[0,1]èŒƒå›´"""
        # CCPOåˆ†æ•°é€šå¸¸åœ¨[-20, 50]èŒƒå›´å†…
        normalized = (score + 20) / 70  # æ˜ å°„åˆ°[0,1]
        return max(0.0, min(1.0, normalized))
    
    def split_dataset(self, preference_pairs: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict]]:
        """åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        total_size = len(preference_pairs)
        train_size = int(total_size * 0.9)  # 90%ç”¨äºè®­ç»ƒ
        
        # éšæœºæ‰“ä¹±
        np.random.seed(42)
        indices = np.random.permutation(total_size)
        
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]
        
        train_pairs = [preference_pairs[i] for i in train_indices]
        test_pairs = [preference_pairs[i] for i in test_indices]
        
        print(f"ğŸ“Š æ•°æ®é›†åˆ†å‰²: è®­ç»ƒé›† {len(train_pairs)}, æµ‹è¯•é›† {len(test_pairs)}")
        return train_pairs, test_pairs
    
    def save_dataset(self, train_pairs: List[Dict], test_pairs: List[Dict]):
        """ä¿å­˜æ•°æ®é›†ä¸ºå¤šç§æ ¼å¼"""
        print(f"ğŸ’¾ ä¿å­˜æ•°æ®é›†...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.args.output_dir, exist_ok=True)
        
        # 1. ä¿å­˜ä¸ºJSON Linesæ ¼å¼ï¼ˆä¾¿äºæ£€æŸ¥ï¼‰
        train_jsonl = f"{self.args.output_dir}/train_prefs.jsonl"
        test_jsonl = f"{self.args.output_dir}/test_prefs.jsonl"
        
        with open(train_jsonl, 'w', encoding='utf-8') as f:
            for pair in train_pairs:
                f.write(json.dumps(pair, ensure_ascii=False) + '\n')
        
        with open(test_jsonl, 'w', encoding='utf-8') as f:
            for pair in test_pairs:
                f.write(json.dumps(pair, ensure_ascii=False) + '\n')
        
        print(f"âœ… JSONLæ ¼å¼ä¿å­˜å®Œæˆ:")
        print(f"   - è®­ç»ƒé›†: {train_jsonl}")
        print(f"   - æµ‹è¯•é›†: {test_jsonl}")
        
        # 2. ä¿å­˜ä¸ºParquetæ ¼å¼ï¼ˆé«˜æ•ˆåŠ è½½ï¼‰
        train_parquet = f"{self.args.output_dir}/train_prefs.parquet"
        test_parquet = f"{self.args.output_dir}/test_prefs.parquet"
        
        import pandas as pd
        pd.DataFrame(train_pairs).to_parquet(train_parquet, index=False)
        pd.DataFrame(test_pairs).to_parquet(test_parquet, index=False)
        
        print(f"âœ… Parquetæ ¼å¼ä¿å­˜å®Œæˆ:")
        print(f"   - è®­ç»ƒé›†: {train_parquet}")
        print(f"   - æµ‹è¯•é›†: {test_parquet}")
        
        # 3. åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        dataset_info = {
            "description": "CCPO Code-Verified Preference Dataset",
            "format": self.args.output_format,
            "train_size": len(train_pairs),
            "test_size": len(test_pairs),
            "features": {
                "chosen": "conversation list" if self.args.output_format == "conversation" else "string",
                "rejected": "conversation list" if self.args.output_format == "conversation" else "string",
                "chosen_probs": "float",
                "chosen_probs_win": "float",
                "chosen_probs_lose": "float",
                "chosen_score": "float",
                "rejected_score": "float",
                "score_difference": "float"
            },
            "splits": {
                "train": f"train_prefs.jsonl",
                "test": f"test_prefs.jsonl"
            },
            "formats": ["jsonl", "parquet"],
            "usage": {
                "jsonl": "Direct loading with datasets.load_dataset('json', data_files='path')",
                "parquet": "Loading with datasets.load_dataset('parquet', data_files='path')"
            }
        }
        
        info_file = f"{self.args.output_dir}/dataset_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶: {info_file}")
        
        # ä¿å­˜å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        stats_file = f"{self.args.output_dir}/processing_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å¤„ç†ç»Ÿè®¡æ–‡ä»¶: {stats_file}")
    
    def print_stats(self):
        """æ‰“å°å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“ˆ CCPOæ•°æ®å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
        print(f"========================")
        print(f"æ€»æ ·æœ¬æ•°: {self.stats['total_samples']}")
        print(f"é«˜è´¨é‡åå¥½å¯¹: {self.stats['high_quality_pairs']}")
        print(f"ä½è´¨é‡åå¥½å¯¹: {self.stats['low_quality_pairs']}")
        print(f"è·³è¿‡æ ·æœ¬æ•°: {self.stats['skipped_samples']}")
        print(f"å¹³å‡åˆ†æ•°å·®å¼‚: {self.stats['avg_score_difference']:.3f}")
        
        if self.stats['total_samples'] > 0:
            quality_rate = self.stats['high_quality_pairs'] / (self.stats['high_quality_pairs'] + self.stats['low_quality_pairs'])
            print(f"é«˜è´¨é‡åå¥½å¯¹æ¯”ä¾‹: {quality_rate:.2%}")
            skip_rate = self.stats['skipped_samples'] / self.stats['total_samples']
            print(f"è·³è¿‡æ ·æœ¬æ¯”ä¾‹: {skip_rate:.2%}")

def main():
    args = parse_arguments()
    
    print(f"ğŸš€ CCPOéªŒè¯ç»“æœå¤„ç†å™¨")
    print(f"===================")
    print(f"è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"æ’åç›®å½•: {args.ranking_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"è¾“å‡ºæ ¼å¼: {args.output_format}")
    print(f"åˆ†æ•°é˜ˆå€¼: {args.score_threshold}")
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = CCPOResultProcessor(args)
    
    try:
        # åŠ è½½æ•°æ®
        data_items, questions, reasoning_processes, ccpo_scores = processor.load_data()
        
        # åˆ›å»ºåå¥½å¯¹
        preference_pairs = processor.create_preference_pairs(questions, reasoning_processes, ccpo_scores)
        
        if not preference_pairs:
            print("âŒ æ²¡æœ‰åˆ›å»ºä»»ä½•åå¥½å¯¹ï¼Œè¯·æ£€æŸ¥åˆ†æ•°é˜ˆå€¼è®¾ç½®")
            return 1
        
        # åˆ†å‰²æ•°æ®é›†
        train_pairs, test_pairs = processor.split_dataset(preference_pairs)
        
        # ä¿å­˜æ•°æ®é›†
        processor.save_dataset(train_pairs, test_pairs)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        processor.print_stats()
        
        print(f"\nâœ… CCPOæ•°æ®å¤„ç†å®Œæˆ!")
        print(f"ğŸ¯ Architecture Bæ•°æ®å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ")
        
        if args.output_format == "conversation":
            print(f"\nğŸ“‹ å¯¹è¯æ ¼å¼ä½¿ç”¨æ–¹æ³•:")
            print(f"   JSONLæ ¼å¼: dataset_mixer: {{'{args.output_dir}/train_prefs.jsonl': 1.0}}")
            print(f"   æ•°æ®æ ¼å¼: chosen/rejected å­—æ®µåŒ…å«å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
        else:
            print(f"\nğŸ“‹ å­—ç¬¦ä¸²æ ¼å¼ä½¿ç”¨æ–¹æ³•:")
            print(f"   JSONLæ ¼å¼: dataset_mixer: {{'{args.output_dir}/train_prefs.jsonl': 1.0}}")
            print(f"   æ•°æ®æ ¼å¼: prompt/chosen/rejected å­—æ®µåŒ…å«æ–‡æœ¬å­—ç¬¦ä¸²")
        
        return 0
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return 1

if __name__ == "__main__":
    exit(main())