from transformers import AutoTokenizer
from datasets import load_dataset,load_from_disk
from vllm import LLM, SamplingParams

import argparse
import torch
import json
import os
from pathlib import Path
import random
import warnings
import numpy as np

warnings.filterwarnings("ignore")


def set_seed(seed=5775709):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model", type=str, default="/data/jiacheng/dylan/aaai/Code-Consistency-Preference-Optimization/cppo/mistralai/Mistral-7B-Instruct-v0.2"
    )
    parser.add_argument("--output_dir", type=str, default="generated/iter1")
    parser.add_argument("--prompts", type=str, default="dylansss/ccpo_math_dataset")
    parser.add_argument("--maxlen", type=int, default=2048)
    parser.add_argument("--pairs", type=int, default=5)
    parser.add_argument("--frac_len", type=int, default=0)
    parser.add_argument("--data_frac", type=int, default=0)
    parser.add_argument("--world_size", type=int, default=1)
    # æ·»åŠ é™åˆ¶å‚æ•°
    parser.add_argument("--limit_samples", type=int, default=None, 
                       help="é™åˆ¶å¤„ç†çš„æ ·æœ¬æ•°é‡ï¼Œç”¨äºæµ‹è¯•")
    return parser.parse_args()


def apply_template(text, tokenizer):
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": text}, {"role": "assistant", "content": "None"}],
        tokenize=False, add_generate_prompt=True
    ).split("None")[0]


def split_prompts(data_items, frac_len, data_frac):
    """ä¿®æ”¹åˆ†ç‰‡å‡½æ•°ï¼Œå¤„ç†å®Œæ•´çš„æ•°æ®é¡¹è€Œä¸ä»…ä»…æ˜¯prompts"""
    if frac_len > 0:
        split_len = frac_len
        if split_len * (data_frac + 1) > len(data_items):
            return data_items[split_len * data_frac:]
        else:
            return data_items[split_len * data_frac: split_len * (data_frac + 1)]
    else:
        return data_items[:]


def main():
    args = parse_arguments()
    model_path = args.model
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½å®Œæ•´æ•°æ®é›†
    print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {args.prompts}")
    data = load_dataset(args.prompts, split="train")
    print(f"âœ… åŸå§‹æ•°æ®é›†å¤§å°: {len(data)}")

    # æ„å»ºå®Œæ•´çš„æ•°æ®é¡¹åˆ—è¡¨ï¼ˆä¿æŒåŸå§‹ç´¢å¼•ï¼‰
    full_data_items = []
    for idx in range(len(data)):
        full_data_items.append({
            'original_index': idx,
            'prompt': data[idx]["prompt"],
            'answer': data[idx]["answer"],
            'source': data[idx].get("source", "unknown")
        })

    # æ ·æœ¬æ•°é‡é™åˆ¶ï¼ˆä¿æŒåŸå§‹ç´¢å¼•ä¿¡æ¯ï¼‰
    if args.limit_samples is not None:
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶å¤„ç†æ ·æœ¬æ•°é‡ä¸º {args.limit_samples}")
        # ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿å¯é‡ç°
        random.seed(42)
        selected_items = random.sample(full_data_items, min(args.limit_samples, len(full_data_items)))
        # æŒ‰åŸå§‹ç´¢å¼•æ’åºï¼Œä¿æŒä¸€å®šçš„é¡ºåº
        selected_items.sort(key=lambda x: x['original_index'])
        full_data_items = selected_items
        print(f"âœ… å·²é€‰æ‹© {len(full_data_items)} ä¸ªæ ·æœ¬è¿›è¡Œå¤„ç†")
        print(f"   åŸå§‹ç´¢å¼•èŒƒå›´: {full_data_items[0]['original_index']} - {full_data_items[-1]['original_index']}")

    # åˆå§‹åŒ–åˆ†è¯å™¨
    if "mistral" in model_path.lower():
        tokenizer = AutoTokenizer.from_pretrained("/data/jiacheng/dylan/aaai/Code-Consistency-Preference-Optimization/cppo/mistralai/Mistral-7B-Instruct-v0.2")
    elif "llama-3" in model_path.lower():
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
    elif "gemma-2" in model_path.lower():
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b-it")
    else:
        raise ValueError("Model not supported")
    tokenizer.pad_token = tokenizer.eos_token

    # åº”ç”¨æ¨¡æ¿åˆ°prompts
    for item in full_data_items:
        item['formatted_prompt'] = apply_template(item['prompt'], tokenizer)

    print(f"ğŸ“Š æ ¼å¼åŒ–åçš„æ•°æ®é¡¹æ•°é‡: {len(full_data_items)}")
    if full_data_items:
        print(f"ç¤ºä¾‹æ ¼å¼åŒ–prompt: {full_data_items[0]['formatted_prompt'][:100]}...")
    
    # åˆ†ç‰‡å¤„ç†
    data_frac, frac_len = args.data_frac, args.frac_len
    processed_data_items = split_prompts(full_data_items, frac_len, data_frac)
    print(f"ğŸ“Š åˆ†ç‰‡åæ•°æ®é¡¹æ•°é‡: {len(processed_data_items)}")

    # æå–promptsç”¨äºç”Ÿæˆ
    prompts_for_generation = [item['formatted_prompt'] for item in processed_data_items]

    # åˆå§‹åŒ–LLM
    llm = LLM(
        model=model_path,
        tensor_parallel_size=args.world_size,
    )

    pairs = args.pairs
    os.makedirs(args.output_dir, exist_ok=True)

    # ä¿å­˜æ•°æ®å…ƒä¿¡æ¯ï¼ˆå…³é”®æ”¹è¿›ï¼ï¼‰
    metadata = {
        'total_original_samples': len(data),
        'selected_samples': len(full_data_items),
        'processed_samples': len(processed_data_items),
        'data_frac': data_frac,
        'frac_len': frac_len,
        'limit_samples': args.limit_samples,
        'pairs': pairs,
        'data_items': [
            {
                'index': i,
                'original_index': item['original_index'],
                'prompt': item['prompt'],
                'answer': item['answer'],
                'source': item['source']
            }
            for i, item in enumerate(processed_data_items)
        ]
    }
    
    metadata_file = f"{args.output_dir}/metadata_{data_frac}.json"
    with open(metadata_file, "w", encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ æ•°æ®å…ƒä¿¡æ¯å·²ä¿å­˜: {metadata_file}")

    # ç”Ÿæˆresponses
    for p in range(pairs):
        print(f"ğŸš€ ç”Ÿæˆç¬¬ {p+1}/{pairs} å¯¹å“åº”...")
        set_seed(p * 50)
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=args.maxlen,
            seed=p * 50,
        )
        response = llm.generate(prompts_for_generation, sampling_params)
        output = list(map(lambda x: x.outputs[0].text, response))
        
        # ä¿å­˜responsesï¼ˆä¿æŒä¸metadataçš„ç´¢å¼•ä¸€è‡´ï¼‰
        response_file = f"{args.output_dir}/responses_{p}.json"
        with open(response_file, "w", encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç¬¬ {p+1} å¯¹å“åº”å·²ä¿å­˜: {response_file}")

    print(f"\nğŸ‰ æ‰€æœ‰å“åº”ç”Ÿæˆå®Œæˆï¼è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - metadata_{data_frac}.json (æ•°æ®å…ƒä¿¡æ¯)")
    for p in range(pairs):
        print(f"   - responses_{p}.json (ç¬¬{p+1}å¯¹å“åº”)")
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   - åŸå§‹æ•°æ®é›†å¤§å°: {metadata['total_original_samples']}")
    print(f"   - é€‰æ‹©å¤„ç†æ ·æœ¬: {metadata['selected_samples']}")
    print(f"   - æœ€ç»ˆå¤„ç†æ ·æœ¬: {metadata['processed_samples']}")
    print(f"   - ç”Ÿæˆå“åº”å¯¹æ•°: {pairs}")


if __name__ == "__main__":
    main()