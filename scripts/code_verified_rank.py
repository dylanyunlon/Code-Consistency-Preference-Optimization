#!/usr/bin/env python3
"""
CCPO Code Verified Ranking Script - Architecture B Implementation
å®ç°æ ¸å¿ƒåˆ›æ–°ï¼šç”¨æœåŠ¡å™¨æŒ‰ç…§7Bæ¨ç†æ€è·¯æ‰§è¡Œä»£ç æ¥éªŒè¯æ¨ç†è´¨é‡
"""

import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from datasets import load_dataset, Dataset
import json
import pandas as pd
import argparse
import os
import numpy as np
import asyncio
import time
from typing import List, Dict, Any, Tuple
from transformers import AutoTokenizer
import logging

# å¼ºåˆ¶å¯¼å…¥æ£€æŸ¥
try:
    from enhanced_answer_extractor_v2 import EnhancedAnswerExtractorV2
    print("âœ… æˆåŠŸå¯¼å…¥å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨V2")
    V2_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨V2: {e}")
    print("âš ï¸  å°†ä½¿ç”¨å†…ç½®å›é€€æ–¹æ³•")
    V2_AVAILABLE = False

# å¯¼å…¥CCPOç‰ˆæ‰§è¡ŒéªŒè¯å™¨
try:
    from execution_verifier import ExecutionVerifier, VerificationResult, VerificationStatus
    print("âœ… æˆåŠŸå¯¼å…¥CCPOæ‰§è¡ŒéªŒè¯å™¨")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥æ‰§è¡ŒéªŒè¯å™¨: {e}")
    raise ImportError("æ‰§è¡ŒéªŒè¯å™¨ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ execution_verifier.py æ–‡ä»¶") from e

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="CCPO Code Verified Ranking - Architecture Bå®ç°")
    parser.add_argument(
        "--model", type=str, 
        default="/data/jiacheng/dylan/iclr2026/Code-Consistency-Preference-Optimization/checkpoints/mistral-7b-instruct-code-verified-ccpo",
        help="Base model path"
    )
    parser.add_argument('--output_dir', type=str, default='generated/iter1')
    parser.add_argument("--numgpu", type=int, default=8)
    parser.add_argument('--prompts', type=str, default='dylansss/ccpo_math_dataset')
    parser.add_argument('--data_frac', type=int, default=0)
    parser.add_argument('--frac_len', type=int, default=0)
    parser.add_argument("--gpu", type=int, default=0)
    parser.add_argument("--pairs", type=int, default=5)
    
    # CCPOéªŒè¯ç›¸å…³å‚æ•°
    parser.add_argument("--verification_url", type=str, default="https://8.134.217.190:17432", 
                       help="ä»£ç æ‰§è¡ŒæœåŠ¡å™¨åœ°å€")
    parser.add_argument("--verification_username", type=str, default="newuser")
    parser.add_argument("--verification_password", type=str, default="newPass123")
    parser.add_argument("--max_concurrent", type=int, default=1, 
                       help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--debug_v2", action="store_true", help="å¯ç”¨è¯¦ç»†è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--verification_sample_rate", type=float, default=0.005, 
                       help="éªŒè¯é‡‡æ ·ç‡")
    
    # é™æµæ§åˆ¶å‚æ•°
    parser.add_argument("--base_delay", type=float, default=15.0, help="åŸºç¡€è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--max_delay", type=float, default=300.0, help="æœ€å¤§é€€é¿å»¶è¿Ÿï¼ˆç§’ï¼‰")
    parser.add_argument("--request_timeout", type=int, default=180, help="å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    
    # é‡è¯•å’Œæ£€æŸ¥ç‚¹æ§åˆ¶
    parser.add_argument("--max_retries", type=int, default=1, help="æœ€å¤§é‡è¯•æ¬¡æ•°")
    parser.add_argument("--checkpoint_interval", type=int, default=1, help="æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”")
    parser.add_argument("--force_restart", action="store_true", help="å¼ºåˆ¶é‡æ–°å¼€å§‹ï¼Œå¿½ç•¥æ£€æŸ¥ç‚¹")
    parser.add_argument("--verification_model", type=str, default="claude-sonnet-4-20250514-all", 
                       help="éªŒè¯æ—¶ä½¿ç”¨çš„æ¨¡å‹")
    
    return parser.parse_args()

def split_prompts(prompts, frac_len, data_frac):
    """åˆ†å‰²æç¤ºæ•°æ®ç”¨äºåˆ†å¸ƒå¼å¤„ç†"""
    if frac_len > 0:
        split_len = frac_len
        if split_len * (data_frac + 1) > len(prompts):
            return prompts[split_len * data_frac:]
        else:
            return prompts[split_len * data_frac: split_len * (data_frac + 1)]
    else:
        return prompts[:]

def apply_template(text, tokenizer):
    """åº”ç”¨èŠå¤©æ¨¡æ¿"""
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": text}, {"role": "assistant", "content": "None"}],
        tokenize=False, add_generate_prompt=True
    ).split("None")[0]

class CCPOCodeVerifiedRanker:
    """
    CCPOä»£ç éªŒè¯æ’åå™¨ - Architecture Bæ ¸å¿ƒå®ç°
    ç”¨æœåŠ¡å™¨æŒ‰ç…§7Bæ¨ç†æ€è·¯æ‰§è¡Œä»£ç æ¥éªŒè¯æ¨ç†è´¨é‡
    """
    
    def __init__(self, args):
        self.args = args
        
        # åˆå§‹åŒ–ç­”æ¡ˆæå–å™¨ï¼ˆå¯é€‰ï¼‰
        if V2_AVAILABLE:
            try:
                self.answer_extractor = EnhancedAnswerExtractorV2(debug=args.debug_v2)
                print(f"âœ… V2å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨åˆå§‹åŒ–å®Œæˆ (è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if args.debug_v2 else 'å…³é—­'})")
            except Exception as e:
                print(f"âš ï¸  V2æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.answer_extractor = None
        else:
            self.answer_extractor = None
        
        # CCPOéªŒè¯ç»Ÿè®¡
        self.ccpo_stats = {
            'total_processed': 0,
            'reasoning_verification_attempted': 0,
            'high_quality_reasoning': 0,
            'low_quality_reasoning': 0,
            'execution_failures': 0,
            'avg_reasoning_confidence': 0.0,
            'cached_results': 0
        }
        
        # é‡è¯•æ§åˆ¶
        self.max_retries = getattr(args, 'max_retries', 1)
        self.checkpoint_interval = getattr(args, 'checkpoint_interval', 1)
        self.processed_cache = set()
    
    def _load_checkpoint(self, checkpoint_path: str) -> Dict[str, Any]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_path) and not self.args.force_restart:
            try:
                with open(checkpoint_path, 'r') as f:
                    checkpoint = json.load(f)
                    print(f"ğŸ“‚ åŠ è½½CCPOæ£€æŸ¥ç‚¹: {len(checkpoint.get('processed_indices', []))} ä¸ªå·²å¤„ç†æ ·æœ¬")
                    return checkpoint
            except Exception as e:
                logger.warning(f"æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
        return {"processed_indices": [], "all_scores": [], "retry_count": 0, "verification_cache": {}}
    
    def _save_checkpoint(self, checkpoint_path: str, data: Dict[str, Any]):
        """ä¿å­˜æ£€æŸ¥ç‚¹ - ä¿®å¤åºåˆ—åŒ–é—®é¢˜"""
        try:
            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
            
            # ç¡®ä¿æ•°æ®å¯åºåˆ—åŒ–
            serializable_data = {}
            for key, value in data.items():
                if key == "verification_cache":
                    # è·³è¿‡verification_cacheä»¥é¿å…åºåˆ—åŒ–é—®é¢˜
                    serializable_data[key] = {}
                else:
                    serializable_data[key] = value
            
            with open(checkpoint_path, 'w') as f:
                json.dump(serializable_data, f, indent=2)
            if self.args.debug_v2:
                print(f"ğŸ’¾ CCPOæ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")

    async def verify_reasoning_with_ccpo_verifier(
        self, 
        questions: List[str], 
        candidates_list: List[Tuple[str, ...]],
        ground_truths: List[str] = None
    ) -> List[np.ndarray]:
        """
        ä½¿ç”¨CCPOéªŒè¯å™¨è¿›è¡Œæ¨ç†è´¨é‡éªŒè¯å’Œæ’å - Architecture Bæ ¸å¿ƒå®ç°
        """
        checkpoint_path = f"ranking/{self.args.output_dir}/ccpo_checkpoint_{self.args.gpu}_{self.args.data_frac}.json"
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = self._load_checkpoint(checkpoint_path)
        processed_indices = set(checkpoint.get("processed_indices", []))
        all_scores = checkpoint.get("all_scores", [])
        retry_count = checkpoint.get("retry_count", 0)
        verification_cache = checkpoint.get("verification_cache", {})
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ£€æŸ¥ç‚¹å†…å®¹
        if self.args.debug_v2:
            print(f"ğŸ” æ£€æŸ¥ç‚¹è°ƒè¯•ä¿¡æ¯:")
            print(f"   æ£€æŸ¥ç‚¹è·¯å¾„: {checkpoint_path}")
            print(f"   å·²å¤„ç†ç´¢å¼•æ•°é‡: {len(processed_indices)}")
            print(f"   å·²å¤„ç†ç´¢å¼•: {sorted(list(processed_indices))[:10]}...")
            print(f"   all_scoresé•¿åº¦: {len(all_scores)}")
            print(f"   é‡è¯•æ¬¡æ•°: {retry_count}")
        
        print(f"ğŸš€ å¼€å§‹CCPOæ¨ç†è´¨é‡éªŒè¯æ’å (Architecture B)")
        print(f"   - å¤„ç†é—®é¢˜æ•°: {len(questions)}")
        print(f"   - æ¯é—®é¢˜æ¨ç†è¿‡ç¨‹æ•°: {len(candidates_list[0]) if candidates_list else 0}")
        print(f"   - Ground Truth: {'å¯ç”¨' if ground_truths else 'ä¸å¯ç”¨'}")
        print(f"   - é‡‡æ ·ç‡: {self.args.verification_sample_rate}")
        print(f"   - éªŒè¯æœåŠ¡å™¨: {self.args.verification_url}")
        print(f"   - éªŒè¯æ¨¡å‹: {self.args.verification_model}")
        print(f"   - å·²å¤„ç†æ ·æœ¬: {len(processed_indices)}/{len(questions)}")
        print(f"   - åŸºç¡€å»¶è¿Ÿ: {self.args.base_delay}ç§’")
        print(f"   - æ ¸å¿ƒåˆ›æ–°: æœåŠ¡å™¨æŒ‰7Bæ¨ç†æ€è·¯æ‰§è¡Œä»£ç éªŒè¯æ¨ç†è´¨é‡")
        
        # åˆå§‹åŒ–åˆ†æ•°åˆ—è¡¨
        if len(all_scores) != len(questions):
            all_scores = [None] * len(questions)
        
        # é‡‡æ ·å†³å®šå“ªäº›æ ·æœ¬è¿›è¡ŒCCPOéªŒè¯
        sample_size = max(1, int(len(questions) * self.args.verification_sample_rate))
        np.random.seed(42 + retry_count)
        sample_indices = set(np.random.choice(len(questions), sample_size, replace=False))
        print(f"   - å®é™…CCPOéªŒè¯æ ·æœ¬æ•°: {sample_size}")
        print(f"   - æ ·æœ¬ç´¢å¼•: {sorted(list(sample_indices))[:10]}..." if len(sample_indices) > 10 else f"   - æ ·æœ¬ç´¢å¼•: {sorted(list(sample_indices))}")
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥è·³è¿‡é€»è¾‘
        if self.args.debug_v2:
            print(f"\nğŸ” è·³è¿‡é€»è¾‘è°ƒè¯•:")
            skip_count = 0
            for idx in range(min(10, len(questions))):  # åªæ£€æŸ¥å‰10ä¸ª
                will_skip = (idx in processed_indices and all_scores[idx] is not None)
                if will_skip:
                    skip_count += 1
                print(f"   ç´¢å¼•{idx}: {'è·³è¿‡' if will_skip else 'å¤„ç†'} (åœ¨processed: {idx in processed_indices}, scoreså­˜åœ¨: {all_scores[idx] is not None})")
            print(f"   å‰10ä¸ªæ ·æœ¬ä¸­è·³è¿‡æ•°é‡: {skip_count}")
        
        consecutive_failures = 0
        max_consecutive_failures = 2
        
        try:
            # ä½¿ç”¨CCPOç‰ˆæ‰§è¡ŒéªŒè¯å™¨
            async with ExecutionVerifier(
                base_url=self.args.verification_url,
                username=self.args.verification_username,
                password=self.args.verification_password,
                debug=self.args.debug_v2,
                timeout=self.args.request_timeout
            ) as verifier:
                print("âœ… CCPOæ‰§è¡ŒéªŒè¯å™¨åˆå§‹åŒ–æˆåŠŸï¼ˆArchitecture Bï¼‰")
                
                # æ¢å¤éªŒè¯ç¼“å­˜
                if verification_cache:
                    verifier.verification_cache.update(verification_cache)
                    self.ccpo_stats['cached_results'] = len(verification_cache)
                    print(f"ğŸ”„ æ¢å¤CCPOéªŒè¯ç¼“å­˜: {len(verification_cache)} ä¸ªç»“æœ")
                
                for idx, (question, candidates) in enumerate(zip(questions, candidates_list)):
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ¯ä¸ªæ ·æœ¬çš„å¤„ç†çŠ¶æ€
                    if self.args.debug_v2 and idx < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                        print(f"\nğŸ” æ ·æœ¬{idx}å¤„ç†çŠ¶æ€:")
                        print(f"   åœ¨processed_indicesä¸­: {idx in processed_indices}")
                        print(f"   all_scores[{idx}]æ˜¯å¦å­˜åœ¨: {all_scores[idx] is not None}")
                        print(f"   åœ¨sample_indicesä¸­: {idx in sample_indices}")
                    
                    # è·³è¿‡å·²å¤„ç†çš„æ ·æœ¬
                    if idx in processed_indices and all_scores[idx] is not None:
                        if self.args.debug_v2 and idx < 5:
                            print(f"   â†’ è·³è¿‡æ ·æœ¬{idx}")
                        continue
                    
                    self.ccpo_stats['total_processed'] += 1
                    
                    if self.args.debug_v2 and idx < 5:
                        print(f"   â†’ å¤„ç†æ ·æœ¬{idx} (total_processed: {self.ccpo_stats['total_processed']})")
                    
                    if idx in sample_indices:
                        # å¯¹é€‰ä¸­çš„æ ·æœ¬è¿›è¡ŒCCPOæ¨ç†éªŒè¯
                        current_ground_truth = ground_truths[idx] if ground_truths and idx < len(ground_truths) else None
                        
                        if self.args.debug_v2:
                            print(f"\nğŸ§  CCPOæ¨ç†éªŒè¯æ ·æœ¬ {idx+1}/{len(questions)}: {question[:50]}...")
                            if current_ground_truth:
                                print(f"   Ground Truth: {current_ground_truth[:50]}...")
                        else:
                            print(f"ğŸ§  CCPOæ¨ç†éªŒè¯æ ·æœ¬ {idx+1}/{len(questions)}")
                        
                        try:
                            # éªŒè¯æ‰€æœ‰å€™é€‰æ¨ç†è¿‡ç¨‹ - CCPOæ ¸å¿ƒé€»è¾‘
                            verification_results = []
                            
                            for candidate_idx, reasoning_process in enumerate(candidates):
                                try:
                                    if self.args.debug_v2:
                                        print(f"  éªŒè¯æ¨ç†è¿‡ç¨‹ {candidate_idx+1}: {reasoning_process[:50]}...")
                                    
                                    # CCPOæ ¸å¿ƒæ–¹æ³•ï¼šéªŒè¯æ¨ç†è¿‡ç¨‹è´¨é‡
                                    result = await verifier.verify_reasoning_process(
                                        question=question,
                                        reasoning_process=reasoning_process,
                                        ground_truth=current_ground_truth,  # ä¼ å…¥ground_truth
                                        use_cache=True,
                                        model=self.args.verification_model
                                    )
                                    
                                    verification_results.append(result)
                                    
                                    # æ¯ä¸ªæ¨ç†è¿‡ç¨‹ä¹‹é—´çš„å»¶è¿Ÿ
                                    await asyncio.sleep(max(self.args.base_delay, 12.0))
                                    
                                except Exception as e:
                                    logger.error(f"æ¨ç†éªŒè¯å¼‚å¸¸ {idx}-{candidate_idx}: {e}")
                                    verification_results.append(VerificationResult(
                                        verified=False,
                                        status=VerificationStatus.ERROR,
                                        ai_answer=None,
                                        code_answer=None,
                                        confidence=0.0,
                                        execution_time=0.0,
                                        code_generated="",
                                        code_id=None,
                                        stdout="",
                                        stderr="",
                                        error_message=str(e),
                                        verification_id=f"error_{idx}_{candidate_idx}",
                                        raw_ai_response="",
                                        reasoning_process=reasoning_process
                                    ))
                            
                            # åŸºäºCCPOéªŒè¯ç»“æœè®¡ç®—æ¨ç†è´¨é‡åˆ†æ•°
                            scores = self._calculate_reasoning_quality_scores(verification_results)
                            
                            # æ›´æ–°CCPOç»Ÿè®¡
                            self.ccpo_stats['reasoning_verification_attempted'] += 1
                            high_quality_count = sum(1 for r in verification_results 
                                                   if isinstance(r, VerificationResult) and r.verified)
                            self.ccpo_stats['high_quality_reasoning'] += high_quality_count
                            self.ccpo_stats['low_quality_reasoning'] += len(verification_results) - high_quality_count
                            
                            consecutive_failures = 0
                            
                            if self.args.debug_v2:
                                print(f"   ğŸ“Š CCPOæ¨ç†éªŒè¯ç»“æœè¯¦æƒ…:")
                                for i, result in enumerate(verification_results):
                                    if isinstance(result, VerificationResult):
                                        quality = "ğŸ¯ é«˜è´¨é‡" if result.verified else "âŒ ä½è´¨é‡"
                                        print(f"   - æ¨ç†{i+1}: {quality} "
                                              f"(ç½®ä¿¡åº¦: {result.confidence:.3f}, "
                                              f"çŠ¶æ€: {result.status.value})")
                                        if result.ai_answer and result.code_answer:
                                            print(f"     æ¨ç†ç­”æ¡ˆ: {result.ai_answer}, æ‰§è¡Œç­”æ¡ˆ: {result.code_answer}")
                                        if result.error_message:
                                            print(f"     é”™è¯¯: {result.error_message[:100]}...")
                                print(f"   ğŸ“ˆ æ¨ç†è´¨é‡å¾—åˆ†: {scores}")
                            
                        except Exception as e:
                            logger.error(f"CCPOæ¨ç†éªŒè¯å¤±è´¥ {idx}: {e}")
                            self.ccpo_stats['execution_failures'] += 1
                            consecutive_failures += 1
                            
                            # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
                            if consecutive_failures >= max_consecutive_failures:
                                logger.error(f"è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤š ({consecutive_failures})ï¼Œæš‚åœ180ç§’...")
                                await asyncio.sleep(180)
                                consecutive_failures = 0
                            
                            # ä½¿ç”¨é»˜è®¤åˆ†æ•°
                            scores = self._get_default_reasoning_scores(len(candidates))
                            
                    else:
                        # æœªé€‰ä¸­çš„æ ·æœ¬ä½¿ç”¨é»˜è®¤è¯„åˆ†ç­–ç•¥
                        scores = self._get_default_reasoning_scores(len(candidates))
                    
                    # ä¿å­˜åˆ†æ•°
                    all_scores[idx] = scores.tolist()
                    processed_indices.add(idx)
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if (idx + 1) % self.checkpoint_interval == 0:
                        checkpoint_data = {
                            "processed_indices": list(processed_indices),
                            "all_scores": all_scores,
                            "retry_count": retry_count,
                            # ä¸ä¿å­˜verification_cacheï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
                            "verification_cache": {}
                        }
                        self._save_checkpoint(checkpoint_path, checkpoint_data)
                        print(f"ğŸ’¾ CCPOæ£€æŸ¥ç‚¹å·²ä¿å­˜: {idx+1}/{len(questions)}")
                    
                    # è¿›åº¦æŠ¥å‘Š
                    if not self.args.debug_v2 and (idx + 1) % 3 == 0:
                        print(f"ğŸ“Š CCPOå¤„ç†è¿›åº¦: {idx+1}/{len(questions)} (é«˜è´¨é‡æ¨ç†: {self.ccpo_stats['high_quality_reasoning']})")
        
        except Exception as e:
            logger.error(f"CCPOéªŒè¯è¿‡ç¨‹å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
            # ä¿å­˜å½“å‰è¿›åº¦
            checkpoint_data = {
                "processed_indices": list(processed_indices),
                "all_scores": all_scores,
                "retry_count": retry_count + 1,
                # ä¸ä¿å­˜verification_cacheï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
                "verification_cache": {}
            }
            self._save_checkpoint(checkpoint_path, checkpoint_data)
            
            if retry_count + 1 < self.max_retries:
                print(f"âš ï¸  CCPOéªŒè¯è¿‡ç¨‹ä¸­æ–­ï¼Œå°†åœ¨é‡è¯•æ—¶ä»æ£€æŸ¥ç‚¹æ¢å¤")
                raise e
            else:
                print(f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨éƒ¨åˆ†ç»“æœ")
        
        # ç¡®ä¿æ‰€æœ‰ä½ç½®éƒ½æœ‰åˆ†æ•°
        for i in range(len(all_scores)):
            if all_scores[i] is None:
                all_scores[i] = self._get_default_reasoning_scores(len(candidates_list[0]) if candidates_list else 5).tolist()
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹ - ä¿®å¤åºåˆ—åŒ–é—®é¢˜
        final_checkpoint = {
            "processed_indices": list(range(len(questions))),
            "all_scores": all_scores,
            "retry_count": retry_count,
            "completed": True,
            # ä¸ä¿å­˜verification_cacheï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
            "verification_cache": {}
        }
        self._save_checkpoint(checkpoint_path, final_checkpoint)
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        if self.ccpo_stats['reasoning_verification_attempted'] > 0:
            quality_rate = self.ccpo_stats['high_quality_reasoning'] / (
                self.ccpo_stats['reasoning_verification_attempted'] * len(candidates_list[0])
            ) if candidates_list else 0
            self.ccpo_stats['avg_reasoning_confidence'] = quality_rate
        
        self._print_ccpo_stats()
        return [np.array(scores) for scores in all_scores]
    
    def _calculate_reasoning_quality_scores(self, verification_results: List[VerificationResult]) -> np.ndarray:
        """
        åŸºäºCCPOéªŒè¯ç»“æœè®¡ç®—æ¨ç†è´¨é‡åˆ†æ•°
        æ ¸å¿ƒåˆ›æ–°ï¼šé«˜è´¨é‡æ¨ç†è¿‡ç¨‹è·å¾—é«˜åˆ†ï¼Œä½è´¨é‡æ¨ç†è¿‡ç¨‹è·å¾—ä½åˆ†
        """
        scores = []
        
        for result in verification_results:
            if result.verified:
                # é«˜è´¨é‡æ¨ç†ï¼šåŸºäºç½®ä¿¡åº¦çš„é«˜åˆ†
                base_score = 30.0  # é«˜åŸºç¡€åˆ†ï¼ˆæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´é«˜ï¼‰
                confidence_bonus = result.confidence * 15.0  # æ›´é«˜çš„ç½®ä¿¡åº¦åŠ æˆ
                execution_bonus = 8.0 if result.execution_time < 10 else 3.0  # æ‰§è¡Œæ•ˆç‡åŠ æˆ
                reasoning_bonus = 5.0  # CCPOæ¨ç†è´¨é‡åŠ æˆ
                score = base_score + confidence_bonus + execution_bonus + reasoning_bonus
                self.ccpo_stats['high_quality_reasoning'] += 1
            else:
                # ä½è´¨é‡æ¨ç†ï¼šåŸºäºé—®é¢˜ç±»å‹çš„æƒ©ç½šåˆ†
                if result.status == VerificationStatus.REASONING_FAILED:
                    score = -15.0  # æ¨ç†è½¬æ¢å¤±è´¥
                elif result.status == VerificationStatus.EXECUTION_FAILED:
                    score = -12.0  # æ‰§è¡Œå¤±è´¥ï¼ˆæ¨ç†æœ‰é—®é¢˜ï¼‰
                elif result.status == VerificationStatus.NO_CODE_GENERATED:
                    score = -8.0  # æ— æ³•ç”Ÿæˆä»£ç ï¼ˆæ¨ç†ä¸æ¸…æ™°ï¼‰
                elif result.status == VerificationStatus.TIMEOUT:
                    score = -5.0  # è¶…æ—¶
                else:
                    score = -3.0  # ç­”æ¡ˆä¸åŒ¹é…ï¼ˆæ¨ç†é€»è¾‘é”™è¯¯ï¼‰
            
            scores.append(score)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å¢å¼ºå·®å¼‚
        scores = np.array(scores)
        
        # CCPOç‰¹æœ‰çš„åˆ†æ•°è°ƒæ•´ï¼šå¼ºåŒ–é«˜è´¨é‡æ¨ç†çš„ä¼˜åŠ¿
        if len(scores) > 1:
            score_range = scores.max() - scores.min()
            if score_range < 5.0:  # å¦‚æœåˆ†æ•°å·®å¼‚å¤ªå°ï¼Œå¢å¼ºå·®å¼‚
                median_score = np.median(scores)
                for i in range(len(scores)):
                    if scores[i] > median_score:
                        scores[i] += 10.0  # å¤§å¹…æå‡é«˜è´¨é‡æ¨ç†
                    elif scores[i] < median_score:
                        scores[i] -= 8.0  # å¤§å¹…é™ä½ä½è´¨é‡æ¨ç†
        
        return scores
    
    def _get_default_reasoning_scores(self, num_candidates: int) -> np.ndarray:
        """è·å–é»˜è®¤æ¨ç†è´¨é‡åˆ†æ•°ï¼ˆæœªéªŒè¯æ ·æœ¬ï¼‰"""
        # ä¸ºæœªéªŒè¯çš„æ ·æœ¬æä¾›è½»å¾®éšæœºåŒ–çš„åˆ†æ•°
        base_scores = np.linspace(-2, 2, num_candidates)
        noise = np.random.normal(0, 0.5, num_candidates)
        return base_scores + noise
    
    def _print_ccpo_stats(self):
        """æ‰“å°CCPOéªŒè¯ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.ccpo_stats
        print("\n" + "="*60)
        print("ğŸ“ˆ CCPOæ¨ç†è´¨é‡éªŒè¯ç»Ÿè®¡æŠ¥å‘Š (Architecture B)")
        print("="*60)
        print(f"æ€»å¤„ç†æ ·æœ¬æ•°: {stats['total_processed']}")
        print(f"æ¨ç†éªŒè¯å°è¯•æ•°: {stats['reasoning_verification_attempted']}")
        print(f"é«˜è´¨é‡æ¨ç†æ•°: {stats['high_quality_reasoning']}")
        print(f"ä½è´¨é‡æ¨ç†æ•°: {stats['low_quality_reasoning']}")
        print(f"æ‰§è¡Œå¤±è´¥æ•°: {stats['execution_failures']}")
        print(f"ç¼“å­˜ç»“æœæ•°: {stats['cached_results']}")
        
        if stats['reasoning_verification_attempted'] > 0:
            quality_rate = stats['high_quality_reasoning'] / (stats['high_quality_reasoning'] + stats['low_quality_reasoning'])
            print(f"æ¨ç†è´¨é‡ç‡: {quality_rate:.2%}")
        
        print(f"å¹³å‡æ¨ç†ç½®ä¿¡åº¦: {stats['avg_reasoning_confidence']:.3f}")
        print(f"CCPOéªŒè¯å™¨çŠ¶æ€: âœ… Architecture Bæ­£å¸¸è¿è¡Œ")
        print(f"æ ¸å¿ƒåˆ›æ–°: æœåŠ¡å™¨æŒ‰7Bæ¨ç†æ€è·¯æ‰§è¡Œä»£ç éªŒè¯æ¨ç†è´¨é‡")
        print("="*60)

async def ccpo_code_verified_ranking(args, questions, candidates, ground_truths):
    """
    CCPOä¸»æ’åå‡½æ•° - Architecture Bå®ç°
    å¢åŠ ground_truthæ”¯æŒç”¨äºæœ€ç»ˆéªŒè¯
    """
    print(f"ğŸš€ å¯åŠ¨CCPOä»£ç éªŒè¯æ’åç³»ç»Ÿ (Architecture B)")
    print(f"   ç‰ˆæœ¬: CCPOæ¨ç†è´¨é‡éªŒè¯ç‰ˆ")
    print(f"   æ ¸å¿ƒåˆ›æ–°: æœåŠ¡å™¨æŒ‰7Bæ¨ç†æ€è·¯æ‰§è¡Œä»£ç ")
    print(f"   éªŒè¯æœåŠ¡å™¨: {args.verification_url}")
    print(f"   æ ·æœ¬æ•°é‡: {len(questions)}")
    print(f"   Ground Truth: {'å¯ç”¨' if ground_truths and ground_truths[0] != 'Unknown' else 'ä¸å¯ç”¨'}")
    
    # åˆå§‹åŒ–CCPOæ’åå™¨
    ranker = CCPOCodeVerifiedRanker(args)
    
    # æ‰§è¡ŒCCPOæ¨ç†è´¨é‡éªŒè¯æ’å
    start_time = time.time()
    ranks = await ranker.verify_reasoning_with_ccpo_verifier(questions, candidates, ground_truths)
    execution_time = time.time() - start_time
    
    print(f"âœ… CCPOæ¨ç†è´¨é‡éªŒè¯æ’åå®Œæˆ")
    print(f"   æ€»è€—æ—¶: {execution_time:.2f}ç§’")
    print(f"   å¹³å‡æ¯æ ·æœ¬: {execution_time/len(questions):.3f}ç§’")
    
    # ä¿å­˜ç»“æœ
    output_path = f"ranking/{args.output_dir}/ccpo_{args.gpu}_{args.data_frac}.npy"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    np.save(output_path, ranks)
    
    print(f"ğŸ’¾ CCPOæ’åç»“æœå·²ä¿å­˜: {output_path}")
    return ranks

async def main(args):
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ CCPOä»£ç éªŒè¯æ’åè„šæœ¬ (Architecture B)")
    print("æ ¸å¿ƒåˆ›æ–°ï¼šç”¨æœåŠ¡å™¨æŒ‰ç…§7Bæ¨ç†æ€è·¯æ‰§è¡Œä»£ç éªŒè¯æ¨ç†è´¨é‡")
    print("="*70)
    
    # éªŒè¯ä¾èµ–
    print("ğŸ” æ£€æŸ¥CCPOä¾èµ–...")
    try:
        from execution_verifier import ExecutionVerifier
        print("âœ… CCPOæ‰§è¡ŒéªŒè¯å™¨å¯ç”¨")
    except ImportError as e:
        print("âŒ CCPOæ‰§è¡ŒéªŒè¯å™¨ä¸å¯ç”¨")
        print("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨å¹¶å·²ä¿®å¤:")
        print("  - execution_verifier.py")
        print("  - enhanced_client_example.py")
        return 1
    
    # å¯¼å…¥æ•°æ®é›†è¿‡æ»¤å™¨ï¼ˆå¯é€‰ï¼‰
    try:
        from improved_verification_filter import filter_dataset_for_verification, print_filter_report
        print("âœ… é€šç”¨è¿‡æ»¤å™¨å¯ç”¨")
        use_filter = False
    except ImportError:
        print("âš ï¸  è¿‡æ»¤å™¨ä¸å¯ç”¨ï¼Œå°†å¤„ç†æ‰€æœ‰æ ·æœ¬ï¼ˆCCPOæ•°å­¦æ•°æ®é›†å·²é¢„è¿‡æ»¤ï¼‰")
        use_filter = False
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“Š åŠ è½½æ•°æ®é›†: {args.prompts}")
    try:
        data = load_dataset(args.prompts, split="train")
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œæ ·æœ¬æ•°: {len(data)}")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    print(f"\nğŸ”§ åˆå§‹åŒ–åˆ†è¯å™¨...")
    if "mistral" in args.model.lower():
        tokenizer = AutoTokenizer.from_pretrained("/data/jiacheng/dylan/aaai/Code-Consistency-Preference-Optimization/cppo/mistralai/Mistral-7B-Instruct-v0.2")
    elif "llama-3" in args.model.lower():
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
    elif "gemma-2" in args.model.lower():
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b-it")
    else:
        print(f"âš ï¸  æœªçŸ¥æ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤Mistralåˆ†è¯å™¨")
        tokenizer = AutoTokenizer.from_pretrained("/data/jiacheng/dylan/aaai/Code-Consistency-Preference-Optimization/cppo/mistralai/Mistral-7B-Instruct-v0.2")
    
    tokenizer.pad_token = tokenizer.eos_token
    print("âœ… åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # å¤„ç†æç¤º
    prompts_all = [apply_template(data[idx]["prompt"], tokenizer) for idx in range(len(data))]
    print(f"âœ… æç¤ºæ¨¡æ¿åº”ç”¨å®Œæˆ")
    if args.debug_v2:
        print(f"ç¤ºä¾‹æç¤º: {prompts_all[0][:100]}...")
    
    # åŠ è½½ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹å’Œå…ƒæ•°æ® - ä¿®å¤æ•°æ®å¯¹åº”å…³ç³»
    print(f"\nğŸ“‚ åŠ è½½ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹å’Œå…ƒæ•°æ®...")
    
    # é¦–å…ˆåŠ è½½å…ƒæ•°æ®æ–‡ä»¶
    metadata_file = f"{args.output_dir}/metadata_{args.data_frac}.json"
    if not os.path.exists(metadata_file):
        print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä½¿ç”¨ä¿®å¤ç‰ˆçš„generate.pyç”Ÿæˆæ•°æ®")
        return 1
    
    with open(metadata_file, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print(f"âœ… å…ƒæ•°æ®åŠ è½½æˆåŠŸ:")
    print(f"   - åŸå§‹æ•°æ®é›†å¤§å°: {metadata['total_original_samples']}")
    print(f"   - å¤„ç†æ ·æœ¬æ•°: {metadata['processed_samples']}")
    print(f"   - ç”Ÿæˆå“åº”å¯¹æ•°: {metadata['pairs']}")
    
    # ä»å…ƒæ•°æ®é‡å»ºæ•°æ®é¡¹
    data_items = metadata['data_items']
    questions_from_metadata = [item['prompt'] for item in data_items]
    answers_from_metadata = [item['answer'] for item in data_items]
    sources_from_metadata = [item['source'] for item in data_items]
    original_indices = [item['original_index'] for item in data_items]
    
    print(f"âœ… ä»å…ƒæ•°æ®é‡å»ºæ•°æ®é¡¹: {len(data_items)} æ¡")
    
    # åŠ è½½å¯¹åº”çš„responses
    pairs = args.pairs
    all_generated = []
    
    for i in range(pairs):
        response_file = f"{args.output_dir}/responses_{i}.json"
        if not os.path.exists(response_file):
            print(f"âŒ å“åº”æ–‡ä»¶ä¸å­˜åœ¨: {response_file}")
            return 1
        
        with open(response_file, 'r', encoding='utf-8') as f:
            gen = json.load(f)
            all_generated.append(gen)
            print(f"âœ… åŠ è½½å“åº”æ–‡ä»¶ {os.path.basename(response_file)} ({len(gen)} æ¡)")
    
    # éªŒè¯æ•°æ®é•¿åº¦ä¸€è‡´æ€§
    expected_length = len(data_items)
    for i, gen in enumerate(all_generated):
        if len(gen) != expected_length:
            print(f"âŒ æ•°æ®é•¿åº¦ä¸åŒ¹é…: metadata({expected_length}) vs responses_{i}({len(gen)})")
            return 1
    
    # å»ºç«‹å®Œæ•´çš„æ•°æ®å¯¹åº”å…³ç³»
    candidates_texts = list(zip(*all_generated))
    
    print(f"âœ… æ•°æ®å¯¹åº”å…³ç³»éªŒè¯å®Œæˆ")
    print(f"   - é—®é¢˜æ•°é‡: {len(questions_from_metadata)}")
    print(f"   - æ¯é—®é¢˜æ¨ç†è¿‡ç¨‹æ•°: {len(candidates_texts[0]) if candidates_texts else 0}")
    print(f"   - Ground Truthå¯ç”¨: âœ…")
    
    # åˆ›å»ºå®Œæ•´çš„éªŒè¯æ•°æ®é›†
    verification_dataset = []
    for idx in range(len(data_items)):
        verification_dataset.append({
            "original_index": original_indices[idx],
            "original_question": questions_from_metadata[idx],
            "reasoning_processes": candidates_texts[idx],
            "ground_truth": answers_from_metadata[idx],
            "source": sources_from_metadata[idx]
        })
    
    print(f"âœ… å®Œæ•´éªŒè¯æ•°æ®é›†æ„å»ºå®Œæˆ: {len(verification_dataset)} æ¡è®°å½•")
    
    # æ˜¾ç¤ºæ•°æ®æ ·æœ¬ï¼ˆç¡®è®¤å¯¹åº”å…³ç³»æ­£ç¡®ï¼‰
    if args.debug_v2 and verification_dataset:
        print(f"\nğŸ“‹ æ•°æ®å¯¹åº”å…³ç³»éªŒè¯:")
        sample = verification_dataset[0]
        print(f"  åŸå§‹ç´¢å¼•: {sample['original_index']}")
        print(f"  åŸå§‹é—®é¢˜: {sample['original_question'][:100]}...")
        print(f"  Ground Truth: {sample['ground_truth'][:100]}...")
        print(f"  æ¨ç†è¿‡ç¨‹æ•°: {len(sample['reasoning_processes'])}")
        for i, reasoning in enumerate(sample['reasoning_processes'][:2]):
            print(f"    æ¨ç†{i+1}: {reasoning[:100]}...")
        print(f"  æ•°æ®æ¥æº: {sample['source']}")
        
        # éªŒè¯ç¬¬äºŒä¸ªæ ·æœ¬
        if len(verification_dataset) > 1:
            sample2 = verification_dataset[1]
            print(f"\n  ç¬¬äºŒä¸ªæ ·æœ¬éªŒè¯:")
            print(f"    åŸå§‹ç´¢å¼•: {sample2['original_index']}")
            print(f"    é—®é¢˜: {sample2['original_question'][:50]}...")
            print(f"    ç­”æ¡ˆ: {sample2['ground_truth'][:50]}...")
    
    # ä¸ºåç»­å¤„ç†å‡†å¤‡æ•°æ®
    questions_for_verification = [item["original_question"] for item in verification_dataset]
    candidates_for_verification = [item["reasoning_processes"] for item in verification_dataset]
    ground_truths = [item["ground_truth"] for item in verification_dataset]
    
    # åˆ†ç‰‡å¤„ç†ï¼ˆåº”ç”¨åˆ°å®Œæ•´éªŒè¯æ•°æ®é›†ï¼‰
    data_frac, frac_len = args.data_frac, args.frac_len
    verification_dataset = split_prompts(verification_dataset, frac_len, data_frac)
    
    # é‡æ–°æå–å¤„ç†åçš„æ•°æ®
    questions_for_verification = [item["original_question"] for item in verification_dataset]
    candidates_for_verification = [item["reasoning_processes"] for item in verification_dataset]
    ground_truths = [item["ground_truth"] for item in verification_dataset]
    
    print(f"âœ… åˆ†ç‰‡åæ ·æœ¬æ•°: {len(verification_dataset)}")
    
    # æ™ºèƒ½è¿‡æ»¤ï¼ˆå¯é€‰ï¼ŒCCPOæ•°å­¦æ•°æ®é›†å·²é¢„è¿‡æ»¤ï¼‰
    if use_filter:
        print(f"\nğŸ” åº”ç”¨æ•°å­¦é—®é¢˜è¿‡æ»¤å™¨...")
        
        max_verification_samples = max(10, int(len(questions_for_verification) * args.verification_sample_rate * 20))
        
        filtered_questions, filtered_candidates, filter_stats = filter_dataset_for_verification(
            questions_for_verification, 
            candidates_for_verification, 
            max_samples=max_verification_samples,
            debug=args.debug_v2
        )
        
        print_filter_report(filter_stats)
        
        if len(filtered_questions) < 5:
            print("âš ï¸  è¿‡æ»¤åæ ·æœ¬æ•°å¤ªå°‘ï¼Œå°†ä½¿ç”¨åŸå§‹CCPOæ•°å­¦æ•°æ®é›†")
            filtered_questions, filtered_candidates = questions_for_verification, candidates_for_verification
        else:
            questions_for_verification, candidates_for_verification = filtered_questions, filtered_candidates
            # åŒæ­¥è°ƒæ•´ground_truths
            ground_truths = ground_truths[:len(questions_for_verification)]
    
    filter_info = "æ•°å­¦é—®é¢˜" if use_filter else "æ— ï¼ˆCCPOæ•°æ®é›†å·²é¢„è¿‡æ»¤ï¼‰"
    
    print(f"\nğŸ¯ å¼€å§‹CCPOæ¨ç†è´¨é‡éªŒè¯æ’å (Architecture B)")
    print(f"   æœ€ç»ˆå¤„ç†æ ·æœ¬æ•°: {len(questions_for_verification)}")
    print(f"   æ¯æ ·æœ¬æ¨ç†è¿‡ç¨‹æ•°: {len(candidates_for_verification[0]) if candidates_for_verification else 0}")
    print(f"   æ™ºèƒ½è¿‡æ»¤: {filter_info}")
    print(f"   Ground Truthå¯ç”¨: {'âœ…' if ground_truths and ground_truths[0] != 'Unknown' else 'âŒ'}")
    print(f"   æ ¸å¿ƒåˆ›æ–°: æœåŠ¡å™¨æŒ‰7Bæ¨ç†æ€è·¯æ‰§è¡Œä»£ç éªŒè¯æ¨ç†è´¨é‡")
    
    # æ‰§è¡ŒCCPOæ¨ç†è´¨é‡éªŒè¯æ’å
    await ccpo_code_verified_ranking(args, questions_for_verification, candidates_for_verification, ground_truths)
    
    print(f"\nâœ… CCPOæ¨ç†è´¨é‡éªŒè¯æ’åå®Œæˆ!")
    print(f"ğŸ‰ Architecture Bæ ¸å¿ƒåˆ›æ–°å·²å®ç°:")
    print(f"   - 7Bæ¨¡å‹ç”Ÿæˆæ¨ç†è¿‡ç¨‹")
    print(f"   - æœåŠ¡å™¨æŒ‰æ¨ç†æ€è·¯ç”Ÿæˆå¹¶æ‰§è¡Œä»£ç ")
    print(f"   - éªŒè¯æ¨ç†è¿‡ç¨‹çš„è´¨é‡")
    print(f"   - ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›é«˜è´¨é‡çš„åå¥½ä¿¡å·")
    return 0

if __name__ == "__main__":
    args = parse_arguments()
    
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    exit_code = asyncio.run(main(args))
    exit(exit_code)